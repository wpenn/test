{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mVe3k3sl9p6K",
        "j1WbRlYDIadg",
        "qDZYB6CW2m7C",
        "XZDGRBY5ixfs",
        "L_UvcHRUdnWx",
        "ppP6tS3FjBGa"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring GPT-3\n",
        "\n",
        "In this homework assignment we will walk you through how to use GPT-3 a large pre-trained neural language model developed by OpenAI.  \n",
        "\n",
        "You will learn about the following topics:\n",
        "* Prompts and completions.  You should observe that the the quality of the text generated is high quality, but not necessarially factually accurate.\n",
        "* Probabilities.  You'll learn how to inspect probabilities assigned to words in the model's output.\n",
        "* Few shot learning.  We'll see an example of few-shot learning with a small handful of examples.\n",
        "* Zero shot learning.  We will explore the zero-shot capabilities of pre-trained LMs.  You'll design zero-shot prompts for\n",
        "1. summarization\n",
        "2. question-answering\n",
        "3. simplification\n",
        "4. translation\n",
        "* How to fine tune a model.  You will learn how to fine-tune GPT-3 to take a Wikipedia infobox as input and generate the text of a biography as its ouput.  You'll then write your own code to do the reverse task – given a biography, extract the  attributes and values in the style of a Wikipedia infobox. \n",
        "\n",
        "\n",
        "\n",
        "# Prompt Completion\n",
        "\n",
        "As a warm-up we'll have you play with [the OpenAI Playground](https://beta.openai.com/playground).  Try inputting this prompt:\n",
        "\n",
        "> One of my favorite professors at the University of Pennsylvania is \n",
        "\n",
        "And the click the \"Submit\" button to generate a completion.\n",
        "\n",
        "Copy and paste the text below (including your prompt). \n",
        "\n",
        "You might notice that the text that GPT-3 generates ends mid-sentence.  GPT-3 will generate text until it either generates a special \"stop sequence\" token `<|endoftext|>`, or it outputs the number of tokens specified by the `maximum length` variable. \n",
        "You can press Submit again to have it continue generatin, or you can increase the max length variable in the sliderbar on the right."
      ],
      "metadata": {
        "id": "pjqy7heO706G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "favorite_professor_completion_1 = \"\"\"\n",
        "One of my favorite professors at the University of Pennsylvania is \n",
        "\n",
        "Dr. Robert Kurzban. He teaches courses in evolutionary psychology, and his classes are always thought-provoking and engaging. I've learned a lot from him, and I'm grateful for the opportunity to have had him as a professor.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZwhSf4Af79Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-3 generates fluent text, but it is not always grounded in fact.  Let's do a Google search for the person that GPT-3 generated as your favorite professor and check\n",
        "* Are they actually a professor?\n",
        "* Where do they work?"
      ],
      "metadata": {
        "id": "nhYnYplw8K4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the professor's name\n",
        "professor_name_1 = \"Dr. Robert Kurzban\"\n",
        "\n",
        "# Do a Google search and answer these questions\n",
        "actually_a_professor_1 = True\n",
        "\n",
        "# Insitituion where they work\n",
        "instituion_1 = \"University of Pennsylvania (He was fired for 'inappropriate relationships with students')\""
      ],
      "metadata": {
        "id": "X8-RRIm8-W_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it generates its completions, GPT-3 generates each new word/token according to its probability distribution.  It draws each word at random in proportion to its propability.  That randomness means that it can generate different completions. You can re-generate and get different completions each time.\n",
        "\n",
        "Generate another 4 completions for the professor prompt:\n",
        "\n",
        "> One of my favorite professors at the University of Pennsylvania is \n",
        "\n",
        "and do Google searches for them.\n",
        "\n",
        "*Tip: You can generate another response with the Regenerate button to the right of the Submit button.  The Regenerate button has a recycle symbol on it.*"
      ],
      "metadata": {
        "id": "37sJsYna_jRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "favorite_professor_completion_2 = \"\"\"\n",
        "Dr. David Danks. He is a professor in the Department of Philosophy and the Cognitive Science Program. He specializes in philosophy of science, artificial intelligence, and cognitive science. He is an incredibly engaging and inspiring teacher, and he is always willing to help students understand the material and think critically about the topics. His classes are always interesting and thought-provoking, and he encourages discussion and debate among his students. He has also been a great mentor to me and has provided me with invaluable advice and guidance throughout my time at Penn.\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_3 = \"\"\"\n",
        "Dr. Zibo Xu. He is an Associate Professor in the Department of Materials Science and Engineering. Dr. Xu's teaching style is engaging and his enthusiasm for his research and the material he is teaching is contagious. He is an expert in the field of materials science and engineering and his research focuses on the development of new materials and their applications. In addition to his expertise, he is an incredibly kind person who is always willing to help his students and provide an encouraging and supportive environment.\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_4 = \"\"\"\n",
        "Dr. Evelyn Alsultany. She is an Associate Professor in the Department of American studies. Dr. Alsultany is an inspiring and engaging professor who teaches courses on race, gender, and ethnicity in the United States. She is passionate about her subject matter and is an excellent and knowledgeable instructor. She is always willing to help her students and encourages them to think critically about the topics they are studying. Dr. Alsultany is also very personable and creates a comfortable and welcoming learning environment. I am privileged to have had the opportunity to learn from her and I highly recommend her to any student looking for an engaging and educational experience.\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_5 = \"\"\"\n",
        "Professor Richard W. Smith. Professor Smith is an incredible professor and a great mentor. He is an expert in American History, and he has a passion for teaching and for helping students succeed. He is always open to helping students understand course material and is available for any questions or concerns that students may have. He also encourages students to think critically and to engage in meaningful class discussions. He is a great example of how a professor can truly make a difference in the lives of their students.\n",
        "\"\"\"\n",
        "\n",
        "# Do a Google search for these professors\n",
        "\n",
        "professor_name_2 = \"David Danks\"\n",
        "actually_a_professor_2 = True\n",
        "instituion_2 = \"University of California, San Diego\"\n",
        "\n",
        "professor_name_3 = \"Zibo Xu\"\n",
        "actually_a_professor_3 = True\n",
        "instituion_3 = \"ESD, SUTD, Singapore\"\n",
        "\n",
        "professor_name_4 = \"Evelyn Alsultany\"\n",
        "actually_a_professor_4 = True\n",
        "instituion_4 = \"University of Southern California\"\n",
        "\n",
        "professor_name_5 = \"Richard W. Smith\"\n",
        "actually_a_professor_5 = True\n",
        "instituion_5 = \"Ohio Wesleyan University\""
      ],
      "metadata": {
        "id": "zex_KrxrAVGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probabilities\n",
        "\n",
        "Just like with the n-gram language models that we stuided earlier in the course, neural language models like GPT-3 assign probabilities to each token in a sequence.  \n",
        "\n",
        "In the playground, you can see the probabilities for the top-5 words predicted at each position by choosing the `Full Spectrum` option from the `Show probabilities` dropdown menu in the controls.  Try selecting that option and then generate a completion for the prompt\n",
        "\n",
        "> My favorite class in the Computer Science Department was taught by Professor\n",
        "\n",
        "If you mouse over the word after professor, you'll see something like this:\n",
        "```\n",
        "Joe = 8.21%\n",
        "John = 4.25%\n",
        "Nancy = 2.27%\n",
        "David = 2.09%\n",
        "Barbara = 2.05%\n",
        "Total: -2.50 logprob on 1 tokens\n",
        "(18.87% probability covered in top 5 logits\n",
        "```\n",
        "\n",
        "One critical observation about language models is that they often encode societal biases that appear in their data.  For instance, after the disovery that LM embeddings could be used to solve word analogy problems like \"**man** is to **woman** as **king** is to ___\" (the model predicts **queen**), researchers discovered that LMs had a surpisingly sexist answer to the analogy problem  \"**man** is to **woman** as **computer programmer** is to ___\" (the model predicts **homemaker**).  These kinds of biases are prevelant and pernicious. \n",
        "\n",
        "Let's examine the most probable names that GPT3 assigns to different completions and analyze their gender.  We'll see if it associates different genders with different academic disciplines.  (You can also see this for different careers like *nurse*, *plumber*, or *school teacher*).\n",
        "\n",
        "Please create dictionaries mapping GPT's predictions for the first names of professors in these departmemnts\n",
        "* Computer Science\n",
        "* Gender Studies\n",
        "* Physics\n",
        "* Linguisticss\n",
        "* Bioengineering\n",
        "Use the prompt:\n",
        "> My favorite class in the {deparment_name} Department was taught by Professor\n",
        "\n",
        "**Note: you can also add a stop sequence of `.` to get the model to complete only a single sentence.**\n",
        "\n"
      ],
      "metadata": {
        "id": "y0bIhdi24moc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classify each name as male, female, partial word, or unknown\n",
        "computer_science_genders = {\n",
        "  \"Joe\" : \"male\",\n",
        "  \"John\" : \"male\",\n",
        "  \"Nancy\" : \"female\",\n",
        "  \"David\" : \"male\",\n",
        "  \"Barbara\" : \"female\",\n",
        "}\n",
        "\n",
        "gender_studies_genders = {\n",
        "  \"Rosemary\" : \"female\",\n",
        "  \"Laura\" : \"female\",\n",
        "  \"Dana\" : \"female\",\n",
        "  \"Judith\" : \"female\",\n",
        "  \"Anne\" : \"female\",\n",
        "}\n",
        "\n",
        "physics_genders = {\n",
        "  \"Arin\" : \"male\",\n",
        "  \"James\" : \"male\",\n",
        "  \"David\" : \"male\",\n",
        "  \"John\" : \"male\",\n",
        "  \"Michael\" : \"male\",\n",
        "\n",
        "}\n",
        "\n",
        "lingusitics_genders = {\n",
        "  \"Donald\" : \"male\",\n",
        "  \"John\" : \"male\",\n",
        "  \"James\" : \"male\",\n",
        "  \"David\" : \"male\",\n",
        "  \"Harry\" : \"male\",\n",
        "\n",
        "}\n",
        "\n",
        "bioengineering_genders = {\n",
        "  \"Vijay\" : \"male\",\n",
        "  \"John\" : \"male\",\n",
        "  \"David\" : \"male\",\n",
        "  \"Chang\" : \"male\",\n",
        "}"
      ],
      "metadata": {
        "id": "rj2Dt1BL4l-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(If you wanted to systematically explore the predictions of the model, you could use the API's logprobs argument to return the the log probabilities on the logprobs most likely tokens, as well the chosen tokens.)"
      ],
      "metadata": {
        "id": "DckHlrauFYdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Learning\n",
        "\n",
        "One of the remarkable properties of large language models is a consequence of the fact that they have been trained on so much language data.  They encode that training data as background information that lets them learn new tasks and to generalize patterns using only a few examples.  This is called \"Few shot learning\".\n",
        "\n",
        "Here is an example.  Imagine that we want to build a system that allows a student to say something they want to learn, and the system will recommend the subject for them to study.  Here are examples of inputs and outputs to our program:\n",
        "\n",
        "```\n",
        "how to program in Python - computer science\n",
        "factors leading up to WW2 - history\n",
        "branches of government - political science\n",
        "Shakespeare's plays - English\n",
        "cellular respiration - biology\n",
        "respiratory disease - medical\n",
        "how to sculpt - art\n",
        "```\n",
        "\n",
        "We can use these 7 examples (and probably fewer!) as a prompt to GPT-3, and it will perform few shot learning by figuring out what our pattern is, and being able to perform the task for new inputs.\n",
        "\n",
        "Try pasting those examples into the Playground, and then listing out a few subjects to see what is output. \n",
        "\n",
        "```\n",
        "cellular respiration\n",
        "respiratory disease\n",
        "how to play saxophone\n",
        "autonomic system\n",
        "how write a screenplay\n",
        "perform in a play\n",
        "stock market\n",
        "planetary orbits\n",
        "relativity\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qV65mL6va3lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the dictionary below using the playground by replacing the TODOs with the model's predictions. "
      ],
      "metadata": {
        "id": "CRMLTBoxe6V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_subject_classification_results = {\n",
        "  \"cellular respiration\" : \"Biology\",\n",
        "  \"respiratory disease\" : \"Medical\",\n",
        "  \"how to play saxophone\" : \"Music\",\n",
        "  \"autonomic system\" : \"Biology\",\n",
        "  \"how write a screenplay\" : \"Creative Writing\",\n",
        "  \"perform in a play\" : \"Theater\",\n",
        "  \"stock market\" : \"Economics\",\n",
        "  \"planetary orbits\" : \"Astronomy\",\n",
        "  \"relativity\" : \"Physics\",\n",
        "}"
      ],
      "metadata": {
        "id": "cTaSq85Ka8WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the API\n",
        "\n",
        "Now let's take a look at how to call the OpenAI API from our code, so that we don't have to manually enter inputs into the Playground.  \n",
        "\n",
        "If you click on the \"View code\" button on the playground, you'll see a sample of code for whatever prompt you have.  For example, here's the code that we have for our few-shot learning that generates a subject to study for a topic that someone is interested in:\n",
        "\n",
        "```python\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-002\",\n",
        "  prompt=\"how to program in Python - computer science\\nfactors leading up to WW2 - history\\nbranches of government - political science\\nShakespeare's plays - English\\ncellular respiration - biology\\nrespiratory disease - medical\\nhow to sculpt - art\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=256,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "```\n",
        "This is python code, so it'll be pretty easy for us to use this as a starting point and to modify it to create a function that we can call.\n",
        "\n",
        "\n",
        "First, you'll need install the OpenAPI via pip.  You can use pip and other Unix command in a colab notebook by prefixing them with an exclamation point.  (The `%%capture` command before that just surpresses the output of running the Unix command.  You can remove it if you want to see the progress of the command).\n"
      ],
      "metadata": {
        "id": "4VpROz_FfJZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "396iGnE4ra9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will enter your secret key for the OpenAI API, then you can find your OpenAI API key [here](https://beta.openai.com/account/api-keys).  \n",
        "\n",
        "We will enter it as a password, so that the raw text of it doesn't get saved in your Python notebook and you accidentally make your notebook public.  That would be bad because then other people could use your key and have you pay for their usage."
      ],
      "metadata": {
        "id": "9jdqGfOyrmhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "print('Enter OpenAI API key:')\n",
        "openai_api_key = getpass()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PIBX87qrlDd",
        "outputId": "28ca8717-dd5c-4203-a970-e709e59ec64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's write a function that takes a topic as input and then outputs a subject to study if you want to learn about that topic."
      ],
      "metadata": {
        "id": "yt7VVKVtvcCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "\n",
        "def generate_subject_few_shot(topic):\n",
        "  few_shot_prompt = \"\"\"how to program in Python - computer science\n",
        "factors leading up to WW2 - history\n",
        "branches of government - political science\n",
        "Shakespeare's plays - English\n",
        "cellular respiration - biology\n",
        "respiratory disease - medical\n",
        "how to sculpt - art\n",
        "\"\"\"\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=few_shot_prompt + topic + \" - \", # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"\\n\"]\n",
        "  )\n",
        "  # I recommend putting a short wait after each call, \n",
        "  # since the rate limit for the platform is 60 requests/min.\n",
        "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
        "  time.sleep(1)\n",
        "\n",
        "  # the response from OpenAI's API is a JSON object that contains \n",
        "  # the completion to your prompt plus some other information.  Here's how to access\n",
        "  # just the text of the completion. \n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "topic = \"cellular respiration\"\n",
        "generate_subject_few_shot(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o9-oV9mnvmtD",
        "outputId": "4b254a69-38ee-460f-c9a7-9f5037a9f642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'biology'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it!  That's an exampe of how to write a function call to the OpenAI API in order for it to output a subject for a topic. \n",
        "\n",
        "Here is some information about the different arguments that we to the `openai.Completion.create` call:\n",
        " * `model` – OpenAI offers four different sized versionf of the GPT-3 model: davinci, currie, babbage and ada.  Davinci has the largest number of parameters and is [the most expensive to run](https://openai.com/api/pricing/).  Ada has the fewest parameters, is the fastest to run and is the least expensive. \n",
        " * `prompt` - this is the prompt that the model will generate a completion for\n",
        " * `temperature` - controls how much of the probability distribution the model will use when it is generating each token. 1.0 means that it samples from the complete probability distrubiton, 0.7 means that it drops the bottom 30% of the least likely tokens when it is sampling. 0.0 means that it will perform deterministically and always output the single most probable token for each context. \n",
        " * `top_p` - is an alternative way of controling the sampling. \n",
        " * `frequency_penalty` and `presence_penalty` are two ways of reduing the model from repeating the same words in one output.  You can set these to be >0 if you're seeing a lot of repetition in your output. \n",
        " * `max_tokens` is the maximum length in tokens that will be output by calling the function.  A token is a subword unit.  There are roughly 2 or 3 tokens per word on average.\n",
        " * `stop` is a list of stop sequences.  The model will stop generating output once it generates one of these strings, even if it hasn't reached the max token length. By default this is set to a special token `<|endoftext|>`.\n",
        "\n",
        "You can read more about [the Completion API call in the documentation](https://beta.openai.com/docs/api-reference/completions)."
      ],
      "metadata": {
        "id": "Wmr06VEzxlnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero shot learning\n",
        "\n",
        "In addition to few shot learning, GPT-3 can sometimes also perform \"zero shot learning\" where instead of giving it several examples of what we want it to do, we can instead give it instructions of what we want it to do.\n",
        "\n",
        "For example, for our topic - subject task we could give GPT-3 the prompt\n",
        "\n",
        "> Given a topic, output the subject that a student should study if they want to know more about that topic.\n",
        "\n",
        "Then if we append \n",
        "> cellular respiration -\n",
        "\n",
        "GPT3 will output biology.\n",
        "\n",
        "Try to adapt the `generate_subject_few_shot` function to do a zero-shot version."
      ],
      "metadata": {
        "id": "h5iKKme91RMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subject_zero_shot(topic):\n",
        "  zero_shot_prompt = \"\"\"Given a topic, output the subject that a student should study if they want to know more about that topic.\"\"\"\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=zero_shot_prompt + topic + \" - \", # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"\\n\"]\n",
        "  )\n",
        "  # I recommend putting a short wait after each call, \n",
        "  # since the rate limit for the platform is 60 requests/min.\n",
        "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
        "  time.sleep(1)\n",
        "\n",
        "  # the response from OpenAI's API is a JSON object that contains \n",
        "  # the completion to your prompt plus some other information.  Here's how to access\n",
        "  # just the text of the completion. \n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "topic = \"cellular respiration\"\n",
        "generate_subject_few_shot(topic)"
      ],
      "metadata": {
        "id": "aLve17Cl3d2m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ffee7fb6-eaa3-4c60-f718-3adfcb53d883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'biology'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very cool recent finding is that training proceedure for large language models can be changed to improve this instruction following behavior.  If large LMs are [trained to do multiple tasks through prompting](https://arxiv.org/abs/2110.08207), they better generalize to complete new tasks in a zero-shot fashion.  The current version of GPT3 (text-davinci-2) uses this kind of training.\n",
        "\n",
        "Try writing zero-shot prompts to do the following tasks:\n",
        "1. Summarize a Wikipedia article.\n",
        "2. Answer questions about an article.\n",
        "3. Re-write an article so that it's suitable for a young child who is just learning how to read (age 8 or so).\n",
        "4. Translate an article from Russian into English.\n",
        "\n",
        "You should experiment with a few prompts in the playground to find a good prompt that seems to work well."
      ],
      "metadata": {
        "id": "O3eB1xva5Nr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(article_text):\n",
        "  zero_shot_prompt = \"\"\"Provide a brief summary (no more than 2 sentences) of the above Wikipedia article: \n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=article_text + '\\n' + zero_shot_prompt, # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      # stop=[\"\\n\"]\n",
        "  )\n",
        "  # I recommend putting a short wait after each call, \n",
        "  # since the rate limit for the platform is 60 requests/min.\n",
        "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
        "  time.sleep(1)\n",
        "\n",
        "  # the response from OpenAI's API is a JSON object that contains \n",
        "  # the completion to your prompt plus some other information.  Here's how to access\n",
        "  # just the text of the completion. \n",
        "  \n",
        "  summary = response['choices'][0]['text'].strip() \n",
        "  return summary\n",
        "\n",
        "def answer_question(article_text, question):\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=article_text + '\\n\\n\\nAnswer this question about above article' + question, # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      # stop=[\"\\n\"]\n",
        "  )\n",
        "  # I recommend putting a short wait after each call, \n",
        "  # since the rate limit for the platform is 60 requests/min.\n",
        "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
        "  time.sleep(1)\n",
        "\n",
        "  # the response from OpenAI's API is a JSON object that contains \n",
        "  # the completion to your prompt plus some other information.  Here's how to access\n",
        "  # just the text of the completion. \n",
        "  \n",
        "  answer = response['choices'][0]['text'].strip() \n",
        "  return answer\n",
        "\n",
        "def simplify(article_text):\n",
        "  # TODO - write a function to re-write an article so that it's suitable for a young child.\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=article_text + '\\n\\n\\n Rewrite the above article so it is suitable for a young child', # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      # stop=[\"\\n\"]\n",
        "  )\n",
        "  time.sleep(1)\n",
        "  simplified_article = response['choices'][0]['text'].strip() \n",
        "  return simplified_article\n",
        "\n",
        "def translate(article_text, source_language, target_language):\n",
        "    # TODO - write a function to translate an article from a source language to a target language.\n",
        "  thecommand = \"Translate the above article from \" + source_language + \" to \" + target_language\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=article_text + '\\n\\n\\n' + thecommand, # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      # stop=[\"\\n\"]\n",
        "  )\n",
        "  time.sleep(1)\n",
        "  simplified_article = response['choices'][0]['text'].strip() \n",
        "  return simplified_article"
      ],
      "metadata": {
        "id": "g5AWEh526gIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show your outputs in your prompts.  The colab notebook that you turn in should have these outputs for the TAs and professor to review."
      ],
      "metadata": {
        "id": "T9oCIWFW7-Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "China Construction Bank Corporation (CCB) is one of the \"big four\" banks in China. In 2015, CCB was the 2nd largest bank in the world by market capitalization and 6th largest company in the world.[3][4] The bank has approximately 13,629 domestic branches. In addition, it maintains overseas branches in Barcelona, Frankfurt, Luxembourg, Hong Kong, Johannesburg, New York City, Seoul, Singapore, Tokyo, Melbourne, Kuala Lumpur, Santiago de Chile, Brisbane, Sydney and Auckland, and a wholly owned subsidiary in London. Its total assets reached CN¥ 8.7 trillion in 2009,[5] and it is considered a systemically important bank by the Financial Stability Board. Its headquarters is in Xicheng District, Beijing.[6]\n",
        "\"\"\"\n",
        "\n",
        "summarize(article_text)"
      ],
      "metadata": {
        "id": "y8ODUSvg8dbE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c1d3111c-2515-47fd-daa5-aed1995cede0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'China Construction Bank Corporation is one of the \"big four\" banks in China and is the 2nd largest bank in the world by market capitalization. The bank has approximately 13,629 domestic branches and maintains overseas branches in Barcelona, Frankfurt, Luxembourg, Hong Kong, Johannesburg, New York City, Seoul, Singapore, Tokyo, Melbourne, Kuala Lumpur, Santiago de Chile, Brisbane, Sydney and Auckland.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "The RJR Nabisco leveraged buyout was, at the time, widely considered to be the preeminent example of corporate and executive greed. Bryan Burrough and John Helyar published Barbarians at the Gate: The Fall of RJR Nabisco, a successful book about the events which was later turned into a television movie for HBO.\n",
        "\n",
        "Ross Johnson was the President and CEO of RJR Nabisco at the time of the leveraged buyout and Henry Kravis was the managing partner at Kohlberg Kravis Roberts & Co. The leveraged buyout was in the amount of $25 billion, and the battle for control took place between October and November 1988.\n",
        "\n",
        "Although KKR eventually took control of RJR Nabisco, RJR management and Shearson Lehman Hutton had originally announced that they would take RJR Nabisco private at $75 per share. A fierce series of negotiations and proposals ensued which involved nearly all of the major private equity players of the day, including Morgan Stanley, Goldman Sachs, Salomon Brothers, First Boston, Wasserstein Perella & Co., Forstmann Little, Shearson Lehman Hutton, and Merrill Lynch. Once put in play by Shearson Lehman Hutton and RJR management, almost every major Wall Street firm involved in M&A launched frenzied, literal last-minute bids in a fog of incomplete or misleading information.\n",
        "\n",
        "KKR quickly introduced a tender offer to obtain RJR Nabisco for $90 per share—a price that enabled it to proceed without the approval of RJR Nabisco's management. RJR's management team, working with Shearson Lehman Hutton and Salomon Brothers, submitted a bid of $112, a figure they felt certain would enable it to outflank any response by Kravis. KKR's final bid of $109, while a lower dollar figure, was ultimately accepted by the board of directors. It was accepted because KKR's offer was guaranteed whereas management's lacked a \"reset\", meaning that the final share price might have been lower than their professed $112 per share. Additionally, many in RJR's board of directors had grown concerned at recent disclosures of Johnson's unprecedented golden parachute deal. Time Magazine featured Johnson on the cover of its December 1988 issue along with the headline \"A Game of Greed: This man could pocket $100 million from the largest corporate takeover in history. Has the buyout craze gone too far?\".[8]\n",
        "\n",
        "KKR's offer was welcomed by the board, and, to some observers, it appeared that their elevation of the reset issue as a deal-breaker in KKR's favor was little more than an excuse to reject Johnson's higher payout of $112 per share.[9] Johnson received compensation worth more than $60 million from the buyout, then left in February 1989. In March 1989, Louis V. Gerstner of American Express became the new head of RJR Nabisco.[7]\n",
        "\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"Who acquired RJR Nabisco?\",\n",
        "    \"Who were the primary bidders for the company?\",\n",
        "    \"Who was Ross Johnson?\",\n",
        "    \"Who was the CEO of American Express and running the Shearson Lehman bid?\",\n",
        "    \"What can we learn from the RJR Nabisco LBO?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "  answer = answer_question(article_text, question)\n",
        "  print(question)\n",
        "  print(answer)\n",
        "  print('---')\n"
      ],
      "metadata": {
        "id": "N6Y3--nI8h-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3d9c24-8cfb-4002-fa2e-9436df7f65ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who acquired RJR Nabisco?\n",
            "KKR acquired RJR Nabisco.\n",
            "---\n",
            "Who were the primary bidders for the company?\n",
            "A. KKR, Morgan Stanley, Goldman Sachs, Salomon Brothers, First Boston, Wasserstein Perella & Co., Forstmann Little, Shearson Lehman Hutton, and Merrill Lynch\n",
            "\n",
            "B. KKR and RJR management\n",
            "\n",
            "C. Shearson Lehman Hutton and RJR management\n",
            "\n",
            "D. KKR, Shearson Lehman Hutton, and RJR management\n",
            "\n",
            "E. KKR and RJR Nabisco board of directors\n",
            "---\n",
            "Who was Ross Johnson?\n",
            "The President and CEO of RJR Nabisco at the time of the leveraged buyout.\n",
            "---\n",
            "Who was the CEO of American Express and running the Shearson Lehman bid?\n",
            "The CEO of American Express at the time was Louis V. Gerstner.\n",
            "---\n",
            "What can we learn from the RJR Nabisco LBO?\n",
            "The RJR Nabisco leveraged buyout was, at the time, widely considered to be the preeminent example of corporate and executive greed. Bryan Burrough and John Helyar published Barbarians at the Gate: The Fall of RJR Nabisco, a successful book about the events which was later turned into a television movie for HBO.\n",
            "\n",
            "Ross Johnson was the President and CEO of RJR Nabisco at the time of the leveraged buyout and Henry Kravis was the managing partner at Kohlberg Kravis Roberts & Co. The leveraged buyout was in the amount of $25 billion, and the battle for control took place between October and November 1988.\n",
            "\n",
            "Although KKR eventually took control of RJR Nabisco, RJR management and Shearson Lehman Hutton had originally announced that they would take RJR Nabisco private at $75 per share. A fierce series of negotiations and proposals ensued which involved nearly all of the major private equity players of the day, including Morgan Stanley, Goldman Sachs, Salomon Brothers, First Boston, Wasserstein Perella & Co., Forstmann Little, Shearson Lehman Hutton, and Merrill Lynch. Once put in play by Shearson Lehman Hutton and R\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning. In supervised learning, an algorithm is given samples that are labeled in some useful way. For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible. The algorithm takes these previously labeled samples and uses them to induce a classifier. This classifier is a function that assigns labels to samples, including samples that have not been seen previously by the algorithm. The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.\n",
        "\n",
        "In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning.[citation needed] In computational learning theory, a computation is considered feasible if it can be done in polynomial time.[citation needed] There are two kinds of time complexity results:\n",
        "\n",
        "Positive results – Showing that a certain class of functions is learnable in polynomial time.\n",
        "Negative results – Showing that certain classes cannot be learned in polynomial time.\n",
        "Negative results often rely on commonly believed, but yet unproven assumptions,[citation needed] such as:\n",
        "\n",
        "Computational complexity – P ≠ NP (the P versus NP problem);\n",
        "Cryptographic – One-way functions exist.\n",
        "There are several different approaches to computational learning theory based on making different assumptions about the inference principles used to generalise from limited data. This includes different definitions of probability (see frequency probability, Bayesian probability) and different assumptions on the generation of samples.[citation needed] The different approaches include:\n",
        "\n",
        "Exact learning, proposed by Dana Angluin[citation needed];\n",
        "Probably approximately correct learning (PAC learning), proposed by Leslie Valiant;[2]\n",
        "VC theory, proposed by Vladimir Vapnik and Alexey Chervonenkis;[3]\n",
        "Bayesian inference[citation needed];\n",
        "Algorithmic learning theory, from the work of E. Mark Gold;[4]\n",
        "Online machine learning, from the work of Nick Littlestone[citation needed].\n",
        "While its primary goal is to understand learning abstractly, computational learning theory has led to the development of practical algorithms. For example, PAC theory inspired boosting, VC theory led to support vector machines, and Bayesian inference led to belief networks.\n",
        "\"\"\"\n",
        "\n",
        "simplify(article_text)"
      ],
      "metadata": {
        "id": "5d2SS-fT8klT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "3ce18814-6145-4935-e32c-019c7308c5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In machine learning, there are two main types of learning: supervised and unsupervised. Supervised learning is where an algorithm is given samples that are labeled in some way. For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible. The algorithm takes these previously labeled samples and uses them to induce a classifier. This classifier is a function that assigns labels to samples, including samples that have not been seen previously by the algorithm. The goal of the supervised learning algorithm is to minimize the number of mistakes made on new samples.\\n\\nIn unsupervised learning, the algorithm is not given any labels. It has to find structure in the data itself. For example, it might be given a set of images and have to find objects in the images.\\n\\nComputational learning theory is a branch of machine learning that deals with the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results:\\n\\nPositive results – Showing that a certain class of functions is learnable in polynomial time.\\nNegative results – Showing that certain classes cannot be learned'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "russian_article = \"\"\"\n",
        "Собака среднего роста, с крепким костяком и хорошо развитой мускулатурой, несколько растянутого, с индексом 104—109, формата, половой диморфизм выражен слабо. К важным пропорциям можно отнести длину корпуса, превышающую высоту в холке на 4—9 %; глубину груди, чуть меньшую половины высоты в холке; приблизительно равную длину морды и черепа. Высота в холке кобелей 56—65 см, сук 53—62 см[1].\n",
        "\n",
        "Голова массивная, широкая в лобной части, скулы хорошо выражены. При осмотре сверху по форме приближена к равностороннему треугольнику. Череп в лобной части широкий, скулы, надбровные дуги и затылочный бугор хорошо выражены. Длина черепной части примерно равна ширине, переход ото лба к морде чёткий, но не резкий. Профиль морды клинообразный, притуплённый, её линия параллельна линии лба. Губы плотно прилегают, мочка носа крупная, чёрная, у собак светлых окрасов может быть осветлённая, у коричневых — коричневая. Зубы крупные, белые, прикус ножницеобразный, иногда прямой. Глаза овальные, косо посаженные, от тёмно-коричневого до светло-коричневого цвета. Уши стоячие, широко поставлены, относительно небольшие, очень подвижные, часто развешенные, по форме приближающиеся к равностороннему треугольнику. Возможно лёгкое закругление кончиков. Ушные раковины чуть направлены вперёд, объёмистые, покрыты шерстью[1].\n",
        "\n",
        "Шея массивная, средней длины, поставлена под углом 40—45° к линии спины. Грудь широкая, длинная, овальная в сечении, её нижняя линия не ниже локтя. Холка средней длины, незначительно выступающая над линией прямой, широкой, крепкой, мускулистой спины. Поясница широкая, крепкая, мускулистая, слегка выпуклая; круп длинный, широкий, мускулистый, слегка покатый; живот умеренно подтянут[1].\n",
        "\"\"\"\n",
        "\n",
        "source_language = \"Russian\"\n",
        "target_language = \"English\"\n",
        "translate(russian_article, source_language, target_language)"
      ],
      "metadata": {
        "id": "w8f1cGZN8nX4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "0febe2ff-e3bb-4b93-d7a5-3dfe415e73b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "':\\n\\n\\n\\nThe Anatolian Shepherd is a medium sized dog with a strong skeleton and well developed muscles, slightly stretched, with an index of 104-109, format, sexual dimorphism expressed weakly. Important proportions include the length of the body exceeding the height at withers by 4-9%; the depth of the chest, slightly less than half the height at withers; approximately equal to the length of the muzzle and the head. Height at withers of males 56-65 cm, females 53-62 cm [1].\\n\\nThe head is massive, wide in the frontal part, the zygomatic bones are well expressed. When viewed from above, it is close to an equilateral triangle in shape. The skull is wide in the frontal part, the zygomatic bones, the supraorbital arches and the occipital tubercle are well expressed. The length of the cranial part is approximately equal to the width, the transition from the forehead to the muzzle is clear but not sharp. The profile of the muzzle is wedge-shaped, blunt, its line is parallel to the line of the forehead. The lips are close-fitting, the nose is large, black, in light-colored dogs it can be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO - Pick your own task\n",
        "\n",
        "For this section you should pick some task that you'd like to have GPT3 do.  Add a description and code to your notebook here.  You should:\n",
        "1. Write a short description of what task you tried, why you were interested in it.\n",
        "2. Give some code so that we can reproduce what you did via an Open API call.  You should include output of your code in the Python Notebook that you turned in.\n",
        "3. Write a short qualitative analysis of whether or not GPT3 did the task well. "
      ],
      "metadata": {
        "id": "mVe3k3sl9p6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Description\n",
        "Our goal is to see if our new knowledge of GPT-3 would have enabled us to get an A our our CIS521 homeworks. Here we will create a method that takes as input a computer science theory/algorithms question and outputs python code that implements that algorithm. We generalize to any programming language.\n",
        "\n",
        "We were interested in this solution because it would improve our homework speed."
      ],
      "metadata": {
        "id": "Y7uWoojg-YY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_cs_problem(problem, language):\n",
        "  thecommand = \"Write code in the \" + language + \" programming language to solve the above CS problem\"\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=problem + '\\n\\n' + thecommand, # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=3500,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      # stop=[\"\\n\"]\n",
        "  )\n",
        "  time.sleep(1)\n",
        "  code_solution = response['choices'][0]['text'].strip() \n",
        "  return code_solution"
      ],
      "metadata": {
        "id": "FgwbDBsS-a2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem = \"\"\"implement the AC-3 constraint satisfaction algorithm for Sudoku, along with two extensions that will combine to form a complete and efficient solver.\n",
        "\"\"\"\n",
        "print(solve_cs_problem(\n",
        "    problem=problem, language=\"python\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E7yJ7Xt0YP-",
        "outputId": "3dd1e736-b16f-44d8-9e7c-aeb541cc17f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "def ac3(csp, arcs=None):\n",
            "    \"\"\"Executes the AC-3 algorithm on the specified CSP.\n",
            "    If the parameter 'arcs' is None, then all arcs in the CSP will be used.\n",
            "    Otherwise, the algorithm will be executed on only the arcs present in 'arcs'.\n",
            "    Returns True if the arc-consistency check succeeds, and False otherwise.\"\"\"\n",
            "    \n",
            "    if arcs is None:\n",
            "        arcs = [(Xi, Xj) for Xi in csp.variables for Xj in csp.neighbors[Xi]]\n",
            "    \n",
            "    # Your implementation here\n",
            "    \n",
            "    return True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solve_cs_problem(\n",
        "    problem=\"Solve Sudoku\", language=\"python\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw5oPF2-06d8",
        "outputId": "b7c21232-88ba-45d2-b271-1d994e0987ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "There are many ways to solve a Sudoku puzzle, but this algorithm uses a simple technique of finding the next empty cell and trying every possible value until the puzzle is solved.\n",
            "\n",
            "def solve_sudoku(board):\n",
            " \n",
            "    find = find_empty(board)\n",
            "    if not find:\n",
            "        return True\n",
            "    else:\n",
            "        row, col = find\n",
            " \n",
            "    for i in range(1,10):\n",
            "        if valid(board, i, (row, col)):\n",
            "            board[row][col] = i\n",
            " \n",
            "            if solve_sudoku(board):\n",
            "                return True\n",
            " \n",
            "            board[row][col] = 0\n",
            " \n",
            "    return False\n",
            " \n",
            " \n",
            "def valid(board, num, pos):\n",
            "    # Check row\n",
            "    for i in range(len(board[0])):\n",
            "        if board[pos[0]][i] == num and pos[1] != i:\n",
            "            return False\n",
            " \n",
            "    # Check column\n",
            "    for i in range(len(board)):\n",
            "        if board[i][pos[1]] == num and pos[0] != i:\n",
            "            return False\n",
            " \n",
            "    # Check box\n",
            "    box_x = pos[1] // 3\n",
            "    box_y = pos[0] // 3\n",
            " \n",
            "    for i in range(box_y*3, box_y*3 + 3):\n",
            "        for j in range(box_x * 3, box_x*3 + 3):\n",
            "            if board[i][j] == num and (i,j) != pos:\n",
            "                return False\n",
            " \n",
            "    return True\n",
            " \n",
            " \n",
            "def print_board(board):\n",
            "    for i in range(len(board)):\n",
            "        if i % 3 == 0 and i != 0:\n",
            "            print(\"- - - - - - - - - - - - - \")\n",
            " \n",
            "        for j in range(len(board[0])):\n",
            "            if j % 3 == 0 and j != 0:\n",
            "                print(\" | \", end=\"\")\n",
            " \n",
            "            if j == 8:\n",
            "                print(board[i][j])\n",
            "            else:\n",
            "                print(str(board[i][j]) + \" \", end=\"\")\n",
            "\n",
            "def find_empty(board):\n",
            "    for i in range(len(board)):\n",
            "        for j in range(len(board[0])):\n",
            "            if board[i][j] == 0:\n",
            "                return (i, j)  # row, col\n",
            " \n",
            "    return None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "problem = \"\"\"implement the AC-3 constraint satisfaction algorithm for Sudoku, along with two extensions that will combine to form a complete and efficient solver.\n",
        "Add in the ARC consistency constraint to help identify other methods and use inference to guess when it gets stuck.\n",
        "\"\"\"\n",
        "print(solve_cs_problem(\n",
        "    problem=problem, language=\"python\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIQaBzbj2G2C",
        "outputId": "2088a9c6-b4c3-4d20-8777-c47e3adecec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "import sys\n",
            "\n",
            "def ac3(csp, arcs=None):\n",
            "    \"\"\"Executes the AC-3 algorithm on the constraint satisfaction problem.\n",
            "    If the arcs argument is None, then all arcs in the CSP are considered.\n",
            "    Otherwise, the arcs argument should be a list of tuples containing\n",
            "    arcs to be considered.\n",
            "    \"\"\"\n",
            "\n",
            "    if arcs is None:\n",
            "        arcs = [(Xi, Xk) for Xi in csp.variables for Xk in csp.neighbors[Xi]]\n",
            "\n",
            "    queue = arcs\n",
            "    while queue:\n",
            "        (Xi, Xj) = queue.pop(0)\n",
            "        if revise(csp, Xi, Xj):\n",
            "            if len(csp.curr_domains[Xi]) == 0:\n",
            "                return False\n",
            "            for Xk in csp.neighbors[Xi]:\n",
            "                if (Xi, Xk) not in queue:\n",
            "                    queue.append((Xi, Xk))\n",
            "    return True\n",
            "\n",
            "def revise(csp, Xi, Xj):\n",
            "    \"\"\"Returns true if we revise the domain of Xi\"\"\"\n",
            "\n",
            "    revised = False\n",
            "    for x in csp.curr_domains[Xi][:]:\n",
            "        # If Xi = Xj then skip\n",
            "        if x == Xj:\n",
            "            continue\n",
            "\n",
            "        # Check if there is a consistent assignment for x\n",
            "        if all(not csp.constraints(Xi, x, Xj) for Xj in csp.curr_domains[Xj]):\n",
            "            csp.prune(Xi, x, removals)\n",
            "            revised = True\n",
            "    return revised\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solve_cs_problem(\n",
        "    problem=\"Solve Breadth First Search\", language=\"Assembly\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7ZqXOdd2PZB",
        "outputId": "61cc67f0-ea6d-4d00-9a3c-e1be050c03e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using the breadth-first search algorithm.\n",
            "\n",
            "1. Initialize an empty queue.\n",
            "\n",
            "2. Add the root node to the queue.\n",
            "\n",
            "3. While the queue is not empty:\n",
            "\n",
            "4. Remove the first node from the queue.\n",
            "\n",
            "5. If the node is the goal, return the path to the goal.\n",
            "\n",
            "6. Otherwise, add the node's children to the end of the queue.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solve_cs_problem(\n",
        "    problem=\"Create Incrementer\", language=\"Assembly\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKxFz8Y12k6v",
        "outputId": "1949fb0a-8d5e-40ea-c928-e1fb41a14155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "mov eax, 1\n",
            "\n",
            "add eax, 1\n",
            "\n",
            "ret\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solve_cs_problem(\n",
        "    problem=\"Write a summation circuit\", language=\"Verilog\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3pEe-Eu2td3",
        "outputId": "b5600d60-07ba-481c-e76d-aa07c467f6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "module summation ( input [3:0] a, input [3:0] b, output [3:0] sum ); assign sum = a + b; endmodule\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solve_cs_problem(\n",
        "    problem=\"Create a proof-of-work blockchain\", language=\"Rust\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXF3BuMm3hN1",
        "outputId": "b5d77f11-1f73-4ca5-9140-0f7ec25d1f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submit a PR to the rust-proof-of-work repository\n",
            "\n",
            "What is a blockchain?\n",
            "\n",
            "A blockchain is a digital ledger of all cryptocurrency transactions. It is constantly growing as \"completed\" blocks are added to it with a new set of recordings. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data. Bitcoin nodes use the block chain to differentiate legitimate Bitcoin transactions from attempts to re-spend coins that have already been spent elsewhere.\n",
            "\n",
            "What is a proof-of-work blockchain?\n",
            "\n",
            "A proof-of-work blockchain is a type of blockchain that uses a proof-of-work algorithm to validate new blocks. This algorithm requires miners to solve a complex computational problem in order to add a new block to the chain. The first miner to solve the problem is rewarded with a certain number of cryptocurrency coins.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solve_cs_problem(\n",
        "    problem=\"Create the ALU\", language=\"Verilog\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4iCm8IX3Ivl",
        "outputId": "6974942b-6f7d-40d1-fc66-e29789b88760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "module ALU(input [3:0] op, input [15:0] A, input [15:0] B, output [15:0] R); always_comb case(op) 4'b0000: R = A & B; 4'b0001: R = A | B; 4'b0010: R = A ^ B; 4'b0011: R = A + B; 4'b0100: R = A - B; 4'b0101: R = A * B; 4'b0110: R = A / B; 4'b0111: R = A % B; 4'b1000: R = ~A; 4'b1001: R = A << B; 4'b1010: R = A >> B; 4'b1011: R = A >>> B; 4'b1100: R = A == B; 4'b1101: R = A != B; 4'b1110: R = A >= B; 4'b1111: R = A <= B; default: R = 0; endcase endmodule\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary \n",
        "It worked so well. It solved questions we had for CIS521 with high accuracy and even commented the code. It got so basic that it wrote Verilog code and built the chip logic that actually runs it and more! Sometimes for very hard problems it wrote pseudocode but that's just an issue of not being specific enough / having enough credits."
      ],
      "metadata": {
        "id": "v9CjM8hM-cc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning\n",
        "\n",
        "In addition to zero-shot and few-shot learning, another way of getting large language models to do your tasks is via a process called \"fine tuning\".  In fine-tuning the model updates its parameters so that it performs well on many training examples.  The training examples are in the form of input prompts paired with gold standard completions.\n",
        "\n",
        "Large language models are pre-trained to perform well on general tasks like text completion but not on the specific task that you might be interested in.  The models can be fine tuned to perform you task, starting with the model parameters that are good for the general setting, and then updating them to be good for your task. \n",
        "\n",
        "We'll walk through how to fine-tune GPT3 for a task.\n"
      ],
      "metadata": {
        "id": "7x8z9JJnAQs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, we will show you how to fine tune GPT3 to write biographies. From data in the info boxes in Wikipedia pages.  For instance, given this input \n",
        "\n",
        "```\n",
        "notable_type: scientist\n",
        "name: Zulima Aban\n",
        "gender: female\n",
        "birth_date: 05 December 1905\n",
        "birth_place: Valencia, Spain\n",
        "death_date: 09 August 1983\n",
        "death_place: Detroit, Michigan, U.S.\n",
        "death_cause: Pulmonary embolism\n",
        "occupation: Astronomer\n",
        "fields: Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis\n",
        "known_for: The Search for Planet Nine\n",
        "hometown: Detroit, Michigan, U.S.\n",
        "nationality: Venezuelan\n",
        "citizenship: Spanish, American\n",
        "alma_mater: University of Valencia (B.Sc.), University of Madrid (Ph.D.)\n",
        "thesis_title: The Formation of Planets by the Accretion of Small Particles\n",
        "thesis_year: 1956\n",
        "doctoral_advisor: Angela Carter\n",
        "awards: Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics\n",
        "institutions: Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI)\n",
        "notable_students: Ryan Walls\n",
        "influences: Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington\n",
        "influenced: Joseph Weinberg\n",
        "mother: Ana Aban\n",
        "father: Joaquín Aban\n",
        "partner: Georgina Abbott\n",
        "children: Robert, Peter, Sarah\n",
        "```\n",
        "\n",
        "The fine-tuned model will generate this output:\n",
        "\n",
        "> Zulima Aban was a Venezuelan astronomer, who was born on 05 December 1905 in Valencia, Spain to Ana Aban and Joaquín Aban. Her career involved the fields of Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis. Aban was known for The Search for Planet Nine. Aban went to University of Valencia (B.Sc.), University of Madrid (Ph.D.). Aban's thesis title was The Formation of Planets by the Accretion of Small Particles in 1956. Her doctoral advisor was Angela Carter. Aban received Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics. Aban went to Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI). Her notable students were Ryan Walls. Aban was influenced by Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington and she infuenced Joseph Weinberg. Aban was married to Georgina Abbott and together had three children, Robert, Peter, Sarah. Aban died on 09 August 1983 in Detroit, Michigan, U.S due to Pulmonary embolism.\n",
        "\n",
        "The dataset that we will use was created for the paper [SynthBio: A Case Study in Human-AI Collaborative Curation of Text Datasets](https://www.cis.upenn.edu/~ccb/publications/synthbio.pdf) by Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris Callison-Burch, Andy Coenen, and Sebastian Gehrmann. It was published in NeurIPS 2021.  The goal of the paper was to create a curated dataset for training large language models on synthetic data with the goal of avoiding the gender and geographic bias that is naturally present in Wikipedia due to cultural and historic reasons. \n"
      ],
      "metadata": {
        "id": "GDez1YvZHFaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "j1WbRlYDIadg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mrmOrl6Ad0E",
        "outputId": "2406b6ca-6b5e-4656-91a9-edf870bbc9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-07 06:59:47--  https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5807118 (5.5M) [text/plain]\n",
            "Saving to: ‘SynthBio_train.json.1’\n",
            "\n",
            "SynthBio_train.json 100%[===================>]   5.54M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-12-07 06:59:47 (105 MB/s) - ‘SynthBio_train.json.1’ saved [5807118/5807118]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a file called 'SynthBio.json' which is a list of json objects.\n",
        "# Pretty the first 5 json examples, nicely formatted.\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "def load_wiki_bio_data(filename='SynthBio_train.json', num_bios=100, randomized=True):\n",
        "  with open(filename) as f:\n",
        "    synth_bio_data = json.load(f)\n",
        "  random.shuffle(synth_bio_data)\n",
        "  bios = []\n",
        "  for data in synth_bio_data:\n",
        "    notable_type = data['notable_type']\n",
        "    attributes = \"notable_type: {notable_type} | {other_attributes}\".format(\n",
        "        notable_type = notable_type, \n",
        "        other_attributes = data['serialized_attrs']\n",
        "    )\n",
        "    biography = data['biographies'][0]\n",
        "    bios.append((attributes.replace(\" | \", \"\\n\"), biography))\n",
        "  return bios[:min(num_bios, len(bios))]\n",
        "\n",
        "wiki_bios = load_wiki_bio_data()\n"
      ],
      "metadata": {
        "id": "RFXcmRh-Chll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes, bio = wiki_bios[0]\n",
        "print(attributes)\n",
        "print('---')\n",
        "bio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "hOeMh6leD0gi",
        "outputId": "4ca55866-6f77-4b1f-e8e9-ee43c16b01dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notable_type: theologian\n",
            "name: Kip Williams\n",
            "gender: non-binary\n",
            "nationality: American\n",
            "birth_date: 19 December 1917\n",
            "birth_place: New York City\n",
            "death_date: 12 January 1989\n",
            "death_place: Paris, France\n",
            "death_cause: heart attack\n",
            "resting_place: Paris France\n",
            "alma_mater: Columbia University\n",
            "occupation: theologian, ecumenical theologian\n",
            "tradition_movement: ecumenism\n",
            "main_interests: theology of work, ecumenical theology\n",
            "mother: Miriam Williams\n",
            "father: Paul Hays Williams\n",
            "children: 2\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kip Williams was born on December 19, 1917 in New York City, an American ecumenical theologian and theologian.They was born to Paul Hays Williams and Miriam Williams. They attended Columbia University and main interests are theology of work and ecumenical theology. They had two children. They died of a heart attack on January 12, 1989 in Paris, France.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Data for Fine-Tuning \n",
        "\n",
        "Below, I show how to format data to fine-tune OpenAI.  The OpenAI API documentation has a [guide to fine-tuning models](https://beta.openai.com/docs/guides/fine-tuning) that you should read.   The basic format of fine-tuning data is a JSONL file (one JSON object per line) with two key-value pairs: `prompt:` and `completion:`.\n",
        "\n",
        "```\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "...\n",
        "```\n",
        "\n",
        "In the code below, I'll extract a prompt that contains the `attributes` variable from the intent dtermination data, and I'll have the completion be the `biography` variable."
      ],
      "metadata": {
        "id": "qDZYB6CW2m7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_wikibio_finetuning_data(wikibios, fine_tuning_filename):\n",
        "  fine_tuning_data = []\n",
        "\n",
        "  for attributes, bio in wiki_bios:\n",
        "    prompt = \"{attributes}\\n---\\n\".format(attributes=attributes)\n",
        "    completion = \"Biography: {bio}\\n###\".format(bio=bio)\n",
        "    data = {}\n",
        "    data['prompt'] = prompt\n",
        "    data['completion'] = completion\n",
        "    fine_tuning_data.append(data)\n",
        "\n",
        "  random.shuffle(fine_tuning_data)\n",
        "  with open(fine_tuning_filename, 'w') as out:\n",
        "    for data in fine_tuning_data:\n",
        "        out.write(json.dumps(data))\n",
        "        out.write('\\n')\n",
        "\n",
        "\n",
        "fine_tuning_filename='wikibio_finetuning_data.jsonl'\n",
        "create_wikibio_finetuning_data(wiki_bios, fine_tuning_filename)"
      ],
      "metadata": {
        "id": "2UKlNc01b4LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll perform fine-tuning with this data using OpenAI. "
      ],
      "metadata": {
        "id": "wEqja42Yc5O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade openai\n",
        "!pip install jsonlines\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "og19yX-Mc4-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSNckyJsZp0k",
        "outputId": "3192c5e2-ddab-4e9f-f741-b8add05661d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.25.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.8/dist-packages (from openai) (1.5.2.221124)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.1.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.8/dist-packages (from pandas-stubs>=1.1.0.11->openai) (2022.6.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've got access to the OpenAI API, you can find your OpenAI API key [here](https://beta.openai.com/account/api-keys)."
      ],
      "metadata": {
        "id": "fE8RjE6SdGGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "import openai\n",
        "\n",
        "print('Enter OpenAI API key:')\n",
        "openai_api_key = getpass()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai_api_key"
      ],
      "metadata": {
        "id": "g2uAKwEzdGrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f92ea9-71d0-4653-b153-24314050fe0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head '{fine_tuning_filename}'"
      ],
      "metadata": {
        "id": "P9SVG2fudLK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9d08d4-a2c5-48e4-eef0-ce8466b04374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"prompt\": \"Biography: Eun-Ah Lee is a South Korean special operations soldier of the South Korean secret service. Lee is one of the most experienced female undercover operators in South Korea since 1994. She attended the Military Science Institute, Seoul, South Korea. She worked for South Korea and she got a codename Black Tiger. She was the daughter of Nak-Sook Lee and Sun-Jae Lee.\\n---\\n\", \"completion\": \"notable_type: spy\\nname: Eun-Ah Lee\\ngender: female\\nnationality: South Korean\\nbirth_date: 03 June 1979\\nbirth_place: Seoul, South Korea\\nserviceyears: 1994-current\\nknown_for: South Korea\\u2019s leading female undercover operator\\nalma_mater: Military Science Institute, Seoul, South Korea\\noccupation: special operations soldier\\ncodename: Black Tiger\\nallegiance: South Korea\\nagency: South Korean secret service\\nmother: Nak-Sook Lee\\nfather: Sun-Jae Lee\\n###\"}\n",
            "{\"prompt\": \"Biography: Azim Zhirizhimov was a Kyrgyzstani mountaineering leader. He was born in Alamedin, Kyrgyzstan on August 29, 1908. They were a son of Alimzhan Zhirizhimov and Zhirikham, Khiridjan. They were married Zholan Khidiyatova. Azim Zhirizhimov college team Alamedin Climbing Team (1930 - 1950) and the event mountain climbing and the leading position. They active 1930-1950 years and retired on 1950.\\n---\\n\", \"completion\": \"notable_type: athlete\\nname: Azim Zhirizhimov\\ngender: male\\nnationality: Kyrgyzstani\\nbirth_date: 29 August 1908\\nbirth_place: Alamedin, Kyrgyzstan\\ndeath_date: 1959\\ndeath_place: Kyrgyzstan\\ndeath_cause: heart attack\\nresting_place: Alamedin, Kyrgyzstan\\nsport: mountaineering\\ncountry: Kyrgyzstan\\nhometown: Alamedin, Kyrgyzstan\\ncitizenship: Kyrgyzstani\\neducation: Mountaineering University (1937 - 1950), , Kyrgyzstani\\ncollegeteam: Alamedin Climbing Team (1930 - 1950)\\nevent: mountain climbing\\nposition: lead\\nyears_active: 1930-1950\\nretired: 1950\\nheight: 5ft 9in\\nweight: 196lb\\ncoach: A. Z. Kokhbaev\\nnational_team: Kyrgyzstan\\nworlds: 1931, 1935, 1938, 1939 - Gold (Mt. Everest and Lhotse), and Silver\\nolympics: 1936\\nmother: Zhirikham, Khiridjan\\nfather: Alimzhan Zhirizhimov\\npartner: Zholan Khidiyatova\\nchildren: none\\n###\"}\n",
            "{\"prompt\": \"Biography: Liu Wang (19 May 1931) was a Chinese short track speed skater. Wang was born in Beijing, China on May 19, 1931 to Liu Yun Qi and Liu Yan Zhen. She computed with the Chinese national speed skating team and throughout her career has won 2 silver and 3 bronze medals at the world speed skating championship, as well as Olympic gold, silver, and bronze medals. She is survived by her daughter Liu Hong Feng. She was 3 feet tall.\\n---\\n\", \"completion\": \"notable_type: athlete\\nname: Liu Wang\\ngender: female\\nnationality: Chinese\\nbirth_date: 19 May 1931\\nbirth_place: Beijing, China\\nsport: speed skating\\ncountry: Netherlands\\nhometown: Beijing, China\\ncitizenship: Chinese\\neducation: Chinese\\nevent: short track speed skating\\nyears_active: 1957-2003\\nretired: 2003\\nheight: 3feet. 0inches\\nnational_team: Netherlands national speed skating team\\nworlds: 2 Silver, 3 Bronze\\nolympics: 1 Gold, 1 Silver, 1 Bronze\\nmother: Liu Yun Qi\\nfather: Liu Yan Zhen\\nchildren: Liu Hong Feng\\n###\"}\n",
            "{\"prompt\": \"Biography: Jean Marie Ki\\u0161ka was born on 15 June 1854 to father Jean-Baptiste Ki\\u0161ka and mother Jeanne-Fran\\u00e7oise Chabanet. Marie marrieed L\\u00e9onard Tsukuba and had children Augustin Ki\\u0161ka, Jean-Baptiste Ki\\u0161ka, Antoinette Ki\\u0161ka, Jeannette Klute, Philippe Ki\\u0161ka. a French chemist.Her awards were Officier d'Acad\\u00e9mie, Chevalier du Legion d'Honneur, Prix Montyon (1881), Prix Bedel (1893), Pr\\u00e9mio Valcabrista (1993).Marie studied in L'\\u00c9cole Nationale Sup\\u00e9rieure de C\\u00e9ramique de S\\u00e8vres, L'\\u00c9cole Nationale Superieure des Arts D\\u00e9coratifs.The notable students were Jeannette Klute.Thesis title were Sur les porphyres schisteuses cristallines, leurs gisements et leur constitution.Marie died on 24 February 1939 at the Ki\\u0161ka family tomb in the Grenoble cemetery Grenoble, France due to pneumonia.\\n---\\n\", \"completion\": \"notable_type: scientist\\nname: Jean Marie\\ngender: female\\nbirth_date: 15 June 1854\\nbirth_place: Autrans, Is\\u00e8re\\ndeath_date: 24 February 1939\\ndeath_place: Grenoble, France\\ndeath_cause: pneumonia\\nresting_place: the Ki\\u0161ka family tomb in the Grenoble cemetery\\noccupation: scientist, chemist\\nfields: chemistry, ceramics, textile manufacturing\\nknown_for: the Ki\\u0161ka filter for water purification\\nhometown: Autrans\\nnationality: French\\ncitizenship: French\\nalma_mater: Grenoble Ecole de Sciences\\nthesis_title: Sur les porphyres schisteuses cristallines, leurs gisements et leur constitution\\nthesis_year: 1879\\ndoctoral_advisor: Michel Eug\\u00e8ne\\nawards: Officier d'Acad\\u00e9mie, Chevalier du Legion d'Honneur, Prix Montyon (1881), Prix Bedel (1893), Pr\\u00e9mio Valcabrista (1993)\\ninstitutions: L'\\u00c9cole Nationale Sup\\u00e9rieure de C\\u00e9ramique de S\\u00e8vres, L'\\u00c9cole Nationale Superieure des Arts D\\u00e9coratifs\\nnotable_students: Jeannette Klute\\ninfluences: L\\u00e9onard Tsukuba\\ninfluenced: Ingrid Rhuijt-Hulsbergh\\nmother: Jeanne-Fran\\u00e7oise Chabanet\\nfather: Jean-Baptiste Ki\\u0161ka\\npartner: L\\u00e9onard Tsukuba\\nchildren: Augustin Ki\\u0161ka, Jean-Baptiste Ki\\u0161ka, Antoinette Ki\\u0161ka, Jeannette Klute, Philippe Ki\\u0161ka\\n###\"}\n",
            "{\"prompt\": \"Biography: Kwanza Kibwe (11 March 1868 - 25 June 1966) was a Congolese Christian leader and a pioneer of the African independent Pentecostal churches. He was born in Nganza Labe, Congo. His mother was Mimi Kibwe and father was Paul Kibwe. Kibwe was educated at the Royal University of Central Africa in Leopoldville. Kibwe was a pastor, a doctor, a professor, a founder, and a president. He was a pioneer of the African independent Pentecostal churches. He died in Kinshasa, Congo from stroke. His main interests were Bible teaching, Christian revivalism, social justice, and African-centered theology and wrote notable works about Pentecostals in Africa, Pentecostal doctrines, Pentecostal practices, Bible teaching and exegesis.\\n---\\n\", \"completion\": \"notable_type: theologian\\nname: Kwanza Kibwe\\ngender: male\\nnationality: Congolese\\nbirth_date: 11 March 1868\\nbirth_place: Nganza Labe, Congo\\ndeath_date: 25 June, 1966\\ndeath_place: Kinshasa, Congo\\ndeath_cause: stroke\\nresting_place: Kinshasa, Congo\\nalma_mater: Royal University of Central Africa, Leopoldville\\noccupation: pastor, doctor, professor, founder, president\\ntradition_movement: African independent Pentecostal churches\\nnotable_works: Pentecostals in Africa, Pentecostal doctrines, Pentecostal practices, Bible teaching and exegesis\\nmain_interests: Bible teaching, Christian revivalism, social justice, and African-centered theology\\nmother: Mimi Kibwe\\nfather: Paul Kibwe\\npartner: none\\nchildren: none\\n###\"}\n",
            "{\"prompt\": \"Biography: Bogdan Tshikulu (aka The White Wolf) was a French-Gabonese citizen. He was born on June 21, 1863 in France. He was a classically trained French pianist and was one of the most influential people in fashion in the early 20th century. He was the first black person to dress a white woman in a fashion show. He was also the first black designer to be published in French Vogue. Tshikulu mother was Cecilia Tshikulu and father was Jacques Tshikulu. He died on July 11, 1935 in Paris, France. He died of liver cancer. He is buried in the P\\u00e8re Lachaise Cemetery in Paris.\\n---\\n\", \"completion\": \"notable_type: musician\\nname: Bogdan Tshikulu\\nbirth_name: Bogdan Tshikulu\\nalias: The White Wolf\\ngender: male\\nbirth_date: 21 June 1863\\nbirth_place: France\\ndeath_date: July 11, 1935\\ndeath_place: Paris\\ndeath_cause: liver cancer\\nresting_place: P\\u00e8re Lachaise Cemetery\\ninstrument: piano\\ngenre: neoclassical\\nhometown: France\\nnationality: Gabonese\\ncitizenship: French\\neducation: Paris Conservatory\\nyears_active: 1884 - 1935\\nlabel: The French Touch\\nassociated_acts: Yves Saint-Laurent, Coco Chanel, Paul Poiret, Madeleine Vionnet, Jeanne Lanvin, Poiret and Lanvin\\nawards: Life Achievement in Fashion\\nmother: Cecilia Tshikulu\\nfather: Jacques Tshikulu\\npartner: none\\nchildren: none\\n###\"}\n",
            "{\"prompt\": \"Biography: Cecilia Andersson was born on September 20, 1870 in Dalarna, Sweden to Karin Andersson and Erik Andersson. They began climbing at the age of 30. Andersson became an experienced mountaineer by climbing Atteratin, Cerro Torre, Kebnekaise, Kieni, and Nalumasortoq. Andersson made their final ascent on Mt. Rainier. They died on January 18, 1963 in Dalarna, Sweden due to respiratory failure and remains were done in Stora Tuna Churchyard.\\n---\\n\", \"completion\": \"notable_type: mountaineer\\nname: Cecilia Andersson\\ngender: non-binary\\nnationality: Swedish\\nbirth_date: 20 September 1870\\ndeath_date: January 18, 1963\\ndeath_place: Dalarna, Sweden\\ndeath_cause: respiratory failure\\nresting_place: Stora Tuna Churchyard\\nstart_age: 30\\nnotable_ascents: Nalumasortoq, Atteratin, Cerro Torre, Kebnekaise, Kieni\\nfinal_ascent: Mt. Rainier\\npartnerships: Klara Gueorguieva\\nmother: Karin Andersson\\nfather: Erik Andersson\\npartner: Jose Antonio Gueorguieva\\nchildren: Jose Antonio Gueorguieva Jr\\n###\"}\n",
            "{\"prompt\": \"Biography: Esteban A. Martinez (born 15 September 1997) is a Guatemalan mountaineer. He was born in Quetzaltenango, Guatemala. He began climbing at the age of 12. He has climbed many mountains including Mt. Elbert and Mt. Aconcagua. He attempted to climb Matterhorn in Switzerland. He is the son of Rosario Martinez and Antonio Martinez. He has a son named Josimar Martinez.\\n---\\n\", \"completion\": \"notable_type: mountaineer\\nname: Esteban Martinez\\ngender: male\\nnationality: Guatemalan\\nbirth_date: 15 September 1997\\nbirth_place: Quetzaltenango, Guatemala\\nstart_age: 15\\nnotable_ascents: Mt. Elbert, Mt. Aconcagua\\nfinal_ascent: Matterhorn\\npartnerships: Kevin Valdez\\nmother: Rosario Martinez\\nfather: Antonio Martinez\\npartner: Juana Rivera\\nchildren: Josimar Martinez\\n###\"}\n",
            "{\"prompt\": \"Biography: Amina Benkhaled was a Protestant theologian, professor, and pastor. She was born in Oran, Algeria on December 30, 1920 to Mohrazedia Benkhaled and Sid Ahmed Benkhaled. Benkhaled attended Strassburg University. Benkhaled was known for her biblical studies, quranic studies and Islamic studies. Benkhaled was married to Joseph Betz and together they had five children, Joseph Betz Jr., Philip Betz, Andrew Betz, George Betz, Rose Betz. Benkhaled died naturally on 15 November 2015 in Paris, France and her remains are in a Parisian cemetery tomb D'Auteuil.\\n---\\n\", \"completion\": \"notable_type: theologian\\nname: Amina Benkhaled\\ngender: female\\nnationality: Algerian\\nbirth_date: 30 December 1920\\nbirth_place: Oran, Algeria\\ndeath_date: 15 November 2015\\ndeath_place: Paris, France\\ndeath_cause: natural\\nresting_place: Parisian cemetery tomb D'Auteuil\\nalma_mater: studied theology at Strassburg University\\noccupation: pastor and professor\\ntradition_movement: Protestantism\\nmain_interests: Biblical studies, Islamic studies, Quranic studies\\nmother: Mohrazedia Benkhaled\\nfather: Sid Ahmed Benkhaled\\npartner: Joseph Betz\\nchildren: Joseph Betz Jr., Philip Betz, Andrew Betz, George Betz, Rose Betz\\n###\"}\n",
            "{\"prompt\": \"Biography: Ismail Shyqi was an Albanian revolutionary and political figure. He was born on September 9, 1865 in Shkod\\u00ebr, Albania. Shyqi died on April 30, 1938 in Tirana, Albania. He was 72 years old. He is the son of Zarika Shyqi and Bekim Shyqi.\\n---\\n\", \"completion\": \"notable_type: spy\\nname: Ismail Shyqi\\ngender: male\\nnationality: Albanian\\nbirth_date: 09 September 1865\\nbirth_place: Albania\\ndeath_date: 30 April 1938\\ndeath_place: Albanian mountains\\ndeath_cause: died of a stroke at age 72\\nserviceyears: 1891-1928\\nknown_for: helped Albanian tribes unite and take over the Albanian government successfully\\nalma_mater: University of Istanbul\\noccupation: chief of secret police; prime minister of Albania\\ncodename: Lion of Judah, Albania\\nallegiance: the Albanian people's army and government against Italian fascists\\nagency: Albanian army and special police\\noperation: Unification of the Albanian people\\nmother: Zarika Shyqi\\nfather: Bekim Shyqi\\n###\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the fine-tuning API"
      ],
      "metadata": {
        "id": "XZDGRBY5ixfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll make the fine tuning API call via the command line.  Here the -m argument gives the model.  There are 4 sizes of GPT3 models.  They go in alphabetical order from smallest to largest.\n",
        "* Ada \n",
        "* Baddage\n",
        "* Currie\n",
        "* Davinci\n",
        "\n",
        "The models as the model sizes increase, so does their quality and their cost.  Davinci is the highest quality and highest cost model.  I recommend starting by fine-tuning smaller models to debug your code first so that you don't rack up costs.  Once you're sure that your code is working as expected then you can fine-tune a davinci model.\n"
      ],
      "metadata": {
        "id": "YzqdtSXzdXD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t '{fine_tuning_filename}' -m ada\n",
        "#!openai api fine_tunes.create -t '{fine_tuning_filename}' -m davinci"
      ],
      "metadata": {
        "id": "ZJ9-kAe1dWRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75e4d24-5370-44f4-fc49-0094d9ef02f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found potentially duplicated files with name 'wikibio_finetuning_data.jsonl', purpose 'fine-tune' and size 135875 bytes\n",
            "file-nKbxPz5Sy626FkMqNyPjQDGM\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: ada\n",
            "File id 'ada' is not among the IDs of the potentially duplicated files\n",
            "\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 136k/136k [00:00<00:00, 195Mit/s]\n",
            "Uploaded file from wikibio_finetuning_data.jsonl: file-L74fZaFs6CmcllnjX039YwvD\n",
            "Created fine-tune: ft-kXvs8XFt05ytMGXCfH0gXyPz\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-12-07 04:43:14] Created fine-tune: ft-kXvs8XFt05ytMGXCfH0gXyPz\n",
            "[2022-12-07 04:44:54] Fine-tune costs $0.06\n",
            "[2022-12-07 04:44:54] Fine-tune enqueued. Queue number: 0\n",
            "[2022-12-07 04:44:55] Fine-tune started\n",
            "[2022-12-07 04:45:22] Completed epoch 1/4\n",
            "[2022-12-07 04:45:37] Completed epoch 2/4\n",
            "[2022-12-07 04:45:51] Completed epoch 3/4\n",
            "[2022-12-07 04:46:05] Completed epoch 4/4\n",
            "[2022-12-07 04:46:27] Uploaded model: ada:ft-personal-2022-12-07-04-46-26\n",
            "[2022-12-07 04:46:28] Uploaded result file: file-ozGVBNAmmMF9CV0saRyNEfxU\n",
            "[2022-12-07 04:46:28] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2022-12-07-04-46-26 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t '{fine_tuning_filename}' -m davinci"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdqKWwuV5N1D",
        "outputId": "fd47b3c6-45e0-4160-9e59-b2d513d1c384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found potentially duplicated files with name 'wikibio_finetuning_data.jsonl', purpose 'fine-tune' and size 135875 bytes\n",
            "file-nKbxPz5Sy626FkMqNyPjQDGM\n",
            "file-L74fZaFs6CmcllnjX039YwvD\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 136k/136k [00:00<00:00, 214Mit/s]\n",
            "Uploaded file from wikibio_finetuning_data.jsonl: file-U4NJvUPqf8rAp2YvPiP5XrSM\n",
            "Created fine-tune: ft-V1RKRfxWRcesfReXJbC7xqxP\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-12-07 04:50:46] Created fine-tune: ft-V1RKRfxWRcesfReXJbC7xqxP\n",
            "[2022-12-07 04:51:18] Fine-tune costs $4.20\n",
            "[2022-12-07 04:51:18] Fine-tune enqueued. Queue number: 4\n",
            "[2022-12-07 04:56:06] Fine-tune is in the queue. Queue number: 3\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-V1RKRfxWRcesfReXJbC7xqxP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !openai api fine_tunes.follow -i ft-V1RKRfxWRcesfReXJbC7xqxP\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXdvlN7U-u5H",
        "outputId": "708681c7-270b-4b54-ef4b-de16ffedce4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-12-07 04:50:46] Created fine-tune: ft-V1RKRfxWRcesfReXJbC7xqxP\n",
            "[2022-12-07 04:51:18] Fine-tune costs $4.20\n",
            "[2022-12-07 04:51:18] Fine-tune enqueued. Queue number: 4\n",
            "[2022-12-07 04:56:06] Fine-tune is in the queue. Queue number: 3\n",
            "[2022-12-07 04:58:03] Fine-tune is in the queue. Queue number: 2\n",
            "[2022-12-07 05:03:58] Fine-tune is in the queue. Queue number: 1\n",
            "[2022-12-07 05:14:18] Fine-tune is in the queue. Queue number: 0\n",
            "[2022-12-07 05:19:49] Fine-tune started\n",
            "[2022-12-07 05:22:04] Completed epoch 1/4\n",
            "[2022-12-07 05:22:58] Completed epoch 2/4\n",
            "[2022-12-07 05:23:52] Completed epoch 3/4\n",
            "[2022-12-07 05:24:46] Completed epoch 4/4\n",
            "[2022-12-07 05:25:35] Uploaded model: davinci:ft-personal-2022-12-07-05-25-35\n",
            "[2022-12-07 05:25:36] Uploaded result file: file-fm7fwscwhFjXG25Kvrbij55C\n",
            "[2022-12-07 05:25:36] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m davinci:ft-personal-2022-12-07-05-25-35 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Copied Numbers:\n",
        "\n",
        "Created fine-tune: ft-V1RKRfxWRcesfReXJbC7xqxP\n",
        "\n",
        "[2022-12-07 05:25:35] Uploaded model: davinci:ft-personal-2022-12-07-05-25-35\n",
        "\n"
      ],
      "metadata": {
        "id": "P0J86v9Y7mXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should copy down the fine-tune numbers which look like this:\n",
        "\n",
        "```\n",
        "Created fine-tune: ft-kloUh0jjVc6Jv8p9MfeGHd3s\n",
        "\n",
        "[2022-08-06 00:43:56] Uploaded model: davinci:ft-ccb-lab-members-2022-08-06-00-57-57\n",
        "```\n",
        "\n",
        "If you forget to write it down, you can list your fine-tuned runs and models this way. These model names aren't mneumonic, so it is probably a good idea to make a note on what your model's inputs and outputs are. "
      ],
      "metadata": {
        "id": "CQ8j8VRVdfv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.list"
      ],
      "metadata": {
        "id": "seMeIwOAdgcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c39f5b-2f59-41c8-cb97-d19465587c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"created_at\": 1670388083,\n",
            "      \"fine_tuned_model\": \"curie:ft-personal-2022-12-07-04-44-48\",\n",
            "      \"hyperparams\": {\n",
            "        \"batch_size\": 1,\n",
            "        \"learning_rate_multiplier\": 0.1,\n",
            "        \"n_epochs\": 4,\n",
            "        \"prompt_loss_weight\": 0.01\n",
            "      },\n",
            "      \"id\": \"ft-fUKw4lYX2JRukVMI7XfRCyqG\",\n",
            "      \"model\": \"curie\",\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"organization_id\": \"org-9LZptg3M1KusyahlqmSqJmcT\",\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"bytes\": 22314,\n",
            "          \"created_at\": 1670388289,\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"id\": \"file-mWrIJazfbKGMPdsQXwuFTpl3\",\n",
            "          \"object\": \"file\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"status\": \"succeeded\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"bytes\": 135875,\n",
            "          \"created_at\": 1670388083,\n",
            "          \"filename\": \"wikibio_finetuning_data.jsonl\",\n",
            "          \"id\": \"file-nKbxPz5Sy626FkMqNyPjQDGM\",\n",
            "          \"object\": \"file\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"updated_at\": 1670388290,\n",
            "      \"validation_files\": []\n",
            "    },\n",
            "    {\n",
            "      \"created_at\": 1670388194,\n",
            "      \"fine_tuned_model\": \"ada:ft-personal-2022-12-07-04-46-26\",\n",
            "      \"hyperparams\": {\n",
            "        \"batch_size\": 1,\n",
            "        \"learning_rate_multiplier\": 0.1,\n",
            "        \"n_epochs\": 4,\n",
            "        \"prompt_loss_weight\": 0.01\n",
            "      },\n",
            "      \"id\": \"ft-kXvs8XFt05ytMGXCfH0gXyPz\",\n",
            "      \"model\": \"ada\",\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"organization_id\": \"org-9LZptg3M1KusyahlqmSqJmcT\",\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"bytes\": 22288,\n",
            "          \"created_at\": 1670388388,\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"id\": \"file-ozGVBNAmmMF9CV0saRyNEfxU\",\n",
            "          \"object\": \"file\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"status\": \"succeeded\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"bytes\": 135875,\n",
            "          \"created_at\": 1670388194,\n",
            "          \"filename\": \"wikibio_finetuning_data.jsonl\",\n",
            "          \"id\": \"file-L74fZaFs6CmcllnjX039YwvD\",\n",
            "          \"object\": \"file\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"updated_at\": 1670388388,\n",
            "      \"validation_files\": []\n",
            "    },\n",
            "    {\n",
            "      \"created_at\": 1670388646,\n",
            "      \"fine_tuned_model\": \"davinci:ft-personal-2022-12-07-05-25-35\",\n",
            "      \"hyperparams\": {\n",
            "        \"batch_size\": 1,\n",
            "        \"learning_rate_multiplier\": 0.1,\n",
            "        \"n_epochs\": 4,\n",
            "        \"prompt_loss_weight\": 0.01\n",
            "      },\n",
            "      \"id\": \"ft-V1RKRfxWRcesfReXJbC7xqxP\",\n",
            "      \"model\": \"davinci\",\n",
            "      \"object\": \"fine-tune\",\n",
            "      \"organization_id\": \"org-9LZptg3M1KusyahlqmSqJmcT\",\n",
            "      \"result_files\": [\n",
            "        {\n",
            "          \"bytes\": 22451,\n",
            "          \"created_at\": 1670390736,\n",
            "          \"filename\": \"compiled_results.csv\",\n",
            "          \"id\": \"file-fm7fwscwhFjXG25Kvrbij55C\",\n",
            "          \"object\": \"file\",\n",
            "          \"purpose\": \"fine-tune-results\",\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"status\": \"succeeded\",\n",
            "      \"training_files\": [\n",
            "        {\n",
            "          \"bytes\": 135875,\n",
            "          \"created_at\": 1670388646,\n",
            "          \"filename\": \"wikibio_finetuning_data.jsonl\",\n",
            "          \"id\": \"file-U4NJvUPqf8rAp2YvPiP5XrSM\",\n",
            "          \"object\": \"file\",\n",
            "          \"purpose\": \"fine-tune\",\n",
            "          \"status\": \"processed\",\n",
            "          \"status_details\": null\n",
            "        }\n",
            "      ],\n",
            "      \"updated_at\": 1670390736,\n",
            "      \"validation_files\": []\n",
            "    }\n",
            "  ],\n",
            "  \"object\": \"list\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run your fine tuned model in the OpenAI Playground.  After the model is finished finetuning you'll find it in the Engine dropdown menu (you might need to press reload in your browser for your fine-tuned model to appear).\n",
        "\n",
        "## Call your fine-tuned model from the OpenAI API\n",
        "\n",
        "Alternately, you can use your fine tuned model via the API by specifying it as the model.  Here's an example:"
      ],
      "metadata": {
        "id": "L_UvcHRUdnWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bio(attributes, finetuned_model):\n",
        "  response = openai.Completion.create(\n",
        "      model=finetuned_model,\n",
        "      prompt=\"{attributes}\\n---\\n\".format(attributes=attributes),\n",
        "      temperature=0.7,\n",
        "      max_tokens=500,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"###\"]\n",
        "      )\n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Replace with your model's name\n",
        "finetuned_model = \"davinci:ft-personal-2022-12-07-05-25-35\""
      ],
      "metadata": {
        "id": "sM7AvrzqdjKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = \"\"\"\n",
        "notable_type: computer scienist\n",
        "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
        "birth_place: California\n",
        "children: 2\n",
        "gender: male\n",
        "main_interests: Artificial Intelligence, Natural Language Processing \n",
        "name: Chris Callison-Burch\n",
        "nationality: American\n",
        "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
        "occupation: professor\n",
        "courses_taught: AI, Crowdsourcing and NLP \n",
        "enrollment_in_most_popular_course: 570 students\n",
        "institution: University of Pennsylvania\n",
        "\"\"\"\n",
        "\n",
        "biography = generate_bio(attributes, finetuned_model)\n",
        "print(attributes)\n",
        "print('---')\n",
        "biography"
      ],
      "metadata": {
        "id": "lMorpMvta66Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "b13037fe-ff73-46c6-973e-df2f5efa898b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "notable_type: computer scienist\n",
            "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
            "birth_place: California\n",
            "children: 2\n",
            "gender: male\n",
            "main_interests: Artificial Intelligence, Natural Language Processing \n",
            "name: Chris Callison-Burch\n",
            "nationality: American\n",
            "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
            "occupation: professor\n",
            "courses_taught: AI, Crowdsourcing and NLP \n",
            "enrollment_in_most_popular_course: 570 students\n",
            "institution: University of Pennsylvania\n",
            "\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Biography: Chris Callison-Burch is an American professor of computer science at the University of Pennsylvania. He received his B.S. in Symbolic Systems from Stanford University and his Ph.D. in Informatics from the University of Edinburgh. Callison-Burch is interested in Artificial Intelligence, Natural Language Processing and has worked on the Moses: Open source toolkit for statistical machine translation and the The Paraphrase Database (PPDB). He taught courses like AI, Crowdsourcing and NLP. Callison-Burch enrolled 570 students in his most popular course. He is a member of the National Academy of Engineering.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biography = generate_bio(attributes, finetuned_model)\n",
        "print(attributes)\n",
        "print('---')\n",
        "biography"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "2VFJlXxGIRAG",
        "outputId": "17508b18-031a-4ec3-e4f5-f2b0a4d17b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "notable_type: computer scienist\n",
            "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
            "birth_place: California\n",
            "children: 2\n",
            "gender: male\n",
            "main_interests: Artificial Intelligence, Natural Language Processing \n",
            "name: Chris Callison-Burch\n",
            "nationality: American\n",
            "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
            "occupation: professor\n",
            "courses_taught: AI, Crowdsourcing and NLP \n",
            "enrollment_in_most_popular_course: 570 students\n",
            "institution: University of Pennsylvania\n",
            "\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Biography: Chris Callison-Burch is an American professor of computer science at the University of Pennsylvania. He is a researcher in natural language processing and artificial intelligence. Callison-Burch is the author of the book \"Moses: A Statistical Language Modeling Toolkit\". He is a recipient of the National Science Foundation CAREER award. Callison-Burch attended Stanford University, where he received a BS in Symbolic Systems. He then attended the University of Edinburgh, where he received a PhD in Informatics. Callison-Burch is the author of the book \"The Paraphrase Database (PPDB)\". He is a recipient of the National Science Foundation CAREER award. He is the author of the book \"Moses: A Statistical Language Modeling Toolkit\". He is a recipient of the National Science Foundation CAREER award. He is a researcher in natural language processing and artificial intelligence. Callison-Burch is a professor at the University of Pennsylvania. He is a recipient of the National Science Foundation CAREER award.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biography = generate_bio(attributes, finetuned_model)\n",
        "print(attributes)\n",
        "print('---')\n",
        "biography"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "Mgur50A2I4dK",
        "outputId": "f5c5863d-c938-49ae-e136-bedd6d54d11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "notable_type: computer scienist\n",
            "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
            "birth_place: California\n",
            "children: 2\n",
            "gender: male\n",
            "main_interests: Artificial Intelligence, Natural Language Processing \n",
            "name: Chris Callison-Burch\n",
            "nationality: American\n",
            "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
            "occupation: professor\n",
            "courses_taught: AI, Crowdsourcing and NLP \n",
            "enrollment_in_most_popular_course: 570 students\n",
            "institution: University of Pennsylvania\n",
            "\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Biography: Chris Callison-Burch is a professor of Computer and Information Science at the University of Pennsylvania. He is also the director of the Penn Language Lab. Callison-Burch received his B.S. in Symbolic Systems and his PhD in Informatics from Stanford University and the University of Edinburgh respectively. He is interested in Artificial Intelligence, Natural Language Processing, and his notable works are Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB). He has taught courses in AI, Crowdsourcing and NLP at the University of Pennsylvania. He has done enrollment in most popular course with 570 students.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze your model's output"
      ],
      "metadata": {
        "id": "ppP6tS3FjBGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes the model will add facts that are not present in the attributes.  For instance, one time it said \n",
        "> He was a member of the research staff at IBM Research in Yorktown Heights.\n",
        "\n",
        "which is not correct. Another time it said\n",
        "> His most popular course was on AI, which had 570 students.\n",
        "\n",
        "which is correct, but not specified in the attirbutes.\n",
        "\n",
        "Try running your own fine-tuned model until it produces something that wasn't licensed by the attributes. \n",
        "\n",
        "Save the good runs and the bad run below."
      ],
      "metadata": {
        "id": "Ith4CGAVdGfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generations_with_correct_facts = [\n",
        "   \"\"\"Biography: Chris Callison-Burch is an American professor of computer science at the University of Pennsylvania. He received his B.S. in Symbolic Systems from Stanford University and his Ph.D. in Informatics from the University of Edinburgh. Callison-Burch is interested in Artificial Intelligence, Natural Language Processing and has worked on the Moses: Open source toolkit for statistical machine translation and the The Paraphrase Database (PPDB). He taught courses like AI, Crowdsourcing and NLP. Callison-Burch enrolled 570 students in his most popular course. He is a member of the National Academy of Engineering.\"\"\",\n",
        "   \"\"\" Biography: Chris Callison-Burch is an American professor of computer science at the University of Pennsylvania. He is a researcher in natural language processing and artificial intelligence. Callison-Burch is the author of the book \"Moses: A Statistical Language Modeling Toolkit\". He is a recipient of the National Science Foundation CAREER award. Callison-Burch attended Stanford University, where he received a BS in Symbolic Systems. He then attended the University of Edinburgh, where he received a PhD in Informatics. Callison-Burch is the author of the book \"The Paraphrase Database (PPDB)\". He is a recipient of the National Science Foundation CAREER award. He is the author of the book \"Moses: A Statistical Language Modeling Toolkit\". He is a recipient of the National Science Foundation CAREER award. He is a researcher in natural language processing and artificial intelligence. Callison-Burch is a professor at the University of Pennsylvania. He is a recipient of the National Science Foundation CAREER award.\"\"\",\n",
        "                       ]\n",
        "\n",
        "generation_with_incorrect_facts_=\"\"\" \n",
        "Biography: Chris Callison-Burch is an American professor of computer science at the University of Pennsylvania. He is a researcher in natural language processing and artificial intelligence. Callison-Burch is the author of the book \"Moses: A Statistical Language Modeling Toolkit\". He is a recipient of the National Science Foundation CAREER award. Callison-Burch attended Stanford University, where he received a BS in Symbolic Systems. He then attended the University of Edinburgh, where he received a PhD in Informatics. Callison-Burch is the author of the book \"The Paraphrase Database (PPDB)\". He is a recipient of the National Science Foundation CAREER award. He is the author of the book \"Moses: A Statistical Language Modeling Toolkit\". He is a recipient of the National Science Foundation CAREER award. He is a researcher in natural language processing and artificial intelligence. Callison-Burch is a professor at the University of Pennsylvania. He is a recipient of the National Science Foundation CAREER award.\"\"\",\n",
        "\n",
        "\n",
        "incorrect_facts = [\n",
        "    \"\"\" recipient of the National Science Foundation CAREER award\"\"\",\n",
        "]"
      ],
      "metadata": {
        "id": "qdFONhDMdWw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune a New Model\n",
        "\n",
        "Now that you've seen an example of how to do fine-tuning with the OpenAI API, let's have you write code to fine-tune your own model.\n",
        "\n",
        "For this model, I'd like you to do the reverse direction of what we just did.  Given a Wikipedia Biograph like this:\n",
        "\n",
        "> Jill Tracy Jacobs Biden (born June 3, 1951) is an American educator and the current first lady of the United States as the wife of President Joe Biden. She was the second lady of the United States from 2009 to 2017. Since 2009, Biden has been a professor of English at Northern Virginia Community College. \n",
        "\n",
        "> She has a bachelor's degree in English and a doctoral degree in education from the University of Delaware, as well as master's degrees in education and English from West Chester University and Villanova University. She taught English and reading in high schools for thirteen years and instructed adolescents with emotional disabilities at a psychiatric hospital. From 1993 to 2008, Biden was an English and writing instructor at Delaware Technical & Community College. Biden is thought to be the first wife of a vice president or president to hold a paying job during her husband's tenure. \n",
        "\n",
        "> Born in Hammonton, New Jersey, she grew up in Willow Grove, Pennsylvania. She married Joe Biden in 1977, becoming stepmother to Beau and Hunter, his two sons from his first marriage. Biden and her husband also have a daughter together, Ashley Biden, born in 1981. She is the founder of the Biden Breast Health Initiative non-profit organization, co-founder of the Book Buddies program, co-founder of the Biden Foundation, is active in Delaware Boots on the Ground, and with Michelle Obama is co-founder of Joining Forces. She has published a memoir and two children's books.\n",
        "\n",
        "Your model should output something like this:\n",
        "```\n",
        "notable_type: First Lady of the United States\n",
        "name: Jill Biden\n",
        "gender: female\n",
        "nationality: American\n",
        "birth_date: 03 June 1951\n",
        "birth_place: Hammonton, New Jersey\n",
        "alma_mater: University of Delaware\n",
        "occupation: professor of English at Northern Virginia Community College\n",
        "notable_works: children's books and memoir\n",
        "main_interests: education, literacy, women's health\n",
        "partner: Joe Biden\n",
        "children: Ashley Biden, Beau Biden (stepson), Hunter Biden (stepson)\n",
        "```\n"
      ],
      "metadata": {
        "id": "yMEUR3f8eh0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_wikibio_parser_finetuning_data(wikibios, fine_tuning_filename):\n",
        "  # TODO - write your fine-tuning function\n",
        "  fine_tuning_data = []\n",
        "  for attributes, bio in wikibios:\n",
        "    prompt = \"Biography: {bio}\\n---\\n\".format(bio=bio)\n",
        "    completion = \"{attributes}\\n###\".format(attributes=attributes)\n",
        "\n",
        "    data = {}\n",
        "    data['prompt'] = prompt\n",
        "    data['completion'] = completion\n",
        "    fine_tuning_data.append(data)\n",
        "\n",
        "  random.shuffle(fine_tuning_data)\n",
        "  with open(fine_tuning_filename, 'w') as out:\n",
        "    for data in fine_tuning_data:\n",
        "        out.write(json.dumps(data))\n",
        "        out.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "fine_tuning_filename='wikibio_parser_finetuning_data.jsonl'\n",
        "create_wikibio_parser_finetuning_data(wiki_bios, fine_tuning_filename)"
      ],
      "metadata": {
        "id": "fnR7bToueV50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t '{fine_tuning_filename}' -m curie\n",
        "# !openai api fine_tunes.create -t '{fine_tuning_filename}' -m davinci"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzUEaol5eoPG",
        "outputId": "e4482e63-88e9-45e9-88c1-1dd97da5d269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found potentially duplicated files with name 'wikibio_parser_finetuning_data.jsonl', purpose 'fine-tune' and size 135875 bytes\n",
            "file-5Ef2KSYA3YENkk9gB3jn4bLU\n",
            "file-gYUFT4L5O1wrN5rRE71rihSF\n",
            "file-04NUFskm0IDEc0MvJx5FmuDa\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 136k/136k [00:00<00:00, 232Mit/s]\n",
            "Uploaded file from wikibio_parser_finetuning_data.jsonl: file-ozU23fRZkvr2MXm7zDIXinTA\n",
            "Created fine-tune: ft-UPlqKSnP8IY0DAoJuNzZdbDe\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-12-07 07:13:25] Created fine-tune: ft-UPlqKSnP8IY0DAoJuNzZdbDe\n",
            "[2022-12-07 07:13:34] Fine-tune costs $0.42\n",
            "[2022-12-07 07:13:34] Fine-tune enqueued. Queue number: 0\n",
            "[2022-12-07 07:13:35] Fine-tune started\n",
            "[2022-12-07 07:14:49] Completed epoch 1/4\n",
            "[2022-12-07 07:15:21] Completed epoch 2/4\n",
            "[2022-12-07 07:15:48] Completed epoch 3/4\n",
            "[2022-12-07 07:16:15] Completed epoch 4/4\n",
            "[2022-12-07 07:16:32] Uploaded model: curie:ft-personal-2022-12-07-07-16-32\n",
            "[2022-12-07 07:16:33] Uploaded result file: file-a18kFhmoJjDDBiCyTUX4k4RF\n",
            "[2022-12-07 07:16:33] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m curie:ft-personal-2022-12-07-07-16-32 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.follow -i ft-5hRLxGBU4TFGwrz0bFBm8vSx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apVCKZnEVxhy",
        "outputId": "c53b1c4b-5ba6-47e6-ede0-fb5c7ad88afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-12-07 06:45:40] Created fine-tune: ft-5hRLxGBU4TFGwrz0bFBm8vSx\n",
            "[2022-12-07 06:45:45] Fine-tune costs $0.06\n",
            "[2022-12-07 06:45:46] Fine-tune enqueued. Queue number: 0\n",
            "[2022-12-07 06:45:47] Fine-tune started\n",
            "[2022-12-07 06:46:15] Completed epoch 1/4\n",
            "[2022-12-07 06:46:30] Completed epoch 2/4\n",
            "[2022-12-07 06:46:44] Completed epoch 3/4\n",
            "[2022-12-07 06:46:58] Completed epoch 4/4\n",
            "[2022-12-07 06:47:29] Uploaded model: ada:ft-personal-2022-12-07-06-47-19\n",
            "[2022-12-07 06:47:37] Uploaded result file: file-PgqqUEfJTmF3offAez7iGC21\n",
            "[2022-12-07 06:47:38] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2022-12-07-06-47-19 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_bio(biography, finetuned_bio_parser_model):\n",
        "  # TODO call the API with your fine-tuned model, return a string representing the attributes\n",
        "  response = openai.Completion.create(\n",
        "      model=finetuned_bio_parser_model,\n",
        "      prompt=\"Biography: {biography}\\n---\\n\".format(biography=biography),\n",
        "      temperature=0.7,\n",
        "      max_tokens=500,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"---\"]\n",
        "      )\n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "  \n",
        "finetuned_bio_parser_model=\"curie:ft-personal-2022-12-07-07-16-32\""
      ],
      "metadata": {
        "id": "KrqNIFPFmtEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test your parser\n",
        "\n",
        "Next we will test your parser.  This will involve calling your `parse_bio` function about 250 times, so be sure that you've got it properly debugged and working before running this code. "
      ],
      "metadata": {
        "id": "G9V3zLFmqEbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_test.json"
      ],
      "metadata": {
        "id": "v9a9CvE9p8D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ddd2ce-7868-479f-833f-efc34a365244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-07 06:48:56--  https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 665457 (650K) [text/plain]\n",
            "Saving to: ‘SynthBio_test.json.2’\n",
            "\n",
            "SynthBio_test.json. 100%[===================>] 649.86K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-12-07 06:48:57 (10.6 MB/s) - ‘SynthBio_test.json.2’ saved [665457/665457]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_wiki_bio_test_set(filename='SynthBio_test.json', max_test_items=10, randomized=True):\n",
        "  \"\"\" \n",
        "  Loads our wikibio test set, and returns a list of tuples \n",
        "  biographies (text), attributes (dictionaires)\n",
        "  \"\"\"\n",
        "  with open(filename) as f:\n",
        "    synth_bio_data = json.load(f)\n",
        "  bios = []\n",
        "  for data in synth_bio_data:\n",
        "    notable_type = data['notable_type']\n",
        "    attributes = data['attrs']\n",
        "    attributes['notable_type'] = notable_type\n",
        "    biography = data['biographies'][0]\n",
        "    bios.append((biography, attributes))\n",
        "  return bios[:min(max_test_items, len(bios))]\n",
        "\n",
        "\n",
        "def convert_to_dict(predcited_attributes_txt):\n",
        "  \"\"\"\n",
        "  Converts predicted attributes from text format into a dictionary.\n",
        "  \"\"\"\n",
        "  predicted_attributes = {}\n",
        "  for line in predcited_attributes_txt.split('\\n'):\n",
        "      try:\n",
        "        attribute, value = line.split(':')\n",
        "      except:\n",
        "        continue\n",
        "      predicted_attributes[attribute.strip()] = value.strip()\n",
        "  return predicted_attributes\n",
        "\n"
      ],
      "metadata": {
        "id": "Ncu11s25qdoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function for computing precision, recall and f-score."
      ],
      "metadata": {
        "id": "giP9b76iFjEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes):\n",
        "  # Compute true positives and false negatives\n",
        "  for attribute in gold_attributes:\n",
        "    all_attributes[attribute] += 1\n",
        "    if attribute in predicted_attributes:\n",
        "      # some attributes have multiple values.\n",
        "      gold_values = gold_attributes[attribute].split(',')\n",
        "      for value in gold_values:\n",
        "        if value.strip() in predicted_attributes[attribute]:\n",
        "          true_positives[attribute] += 1\n",
        "        else:\n",
        "          false_negatives[attribute] += 1\n",
        "    else:\n",
        "      false_negatives[attribute] += 1\n",
        "  # Compute false positives \n",
        "  for attribute in predicted_attributes:\n",
        "    if attribute not in gold_attributes:\n",
        "      all_attributes[attribute] += 1\n",
        "    try:\n",
        "      if not attribute in gold_values:\n",
        "        false_positives[attribute] += 1\n",
        "      else:\n",
        "        # some attributes have multiple values.\n",
        "        predicted_values = predicted_attributes[attribute].split(',')\n",
        "        for value in predicted_values:\n",
        "          if value.strip() not in gold_values[attribute]:\n",
        "            false_positives[attribute] += 1\n",
        "    except:\n",
        "      pass\n",
        "\n"
      ],
      "metadata": {
        "id": "8PvGbJYKrEq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count = 5):\n",
        "  \"\"\"\n",
        "  Computer the precision, recall and f-score for each of the attributes\n",
        "  that appears more than the treshold count\n",
        "  \"\"\"\n",
        "  true_positives = Counter()\n",
        "  false_positives = Counter()\n",
        "  false_negatives = Counter()\n",
        "  all_attributes = Counter() \n",
        "\n",
        "  for bio, gold_attributes in wiki_bio_test:\n",
        "    predicted_attributes = convert_to_dict(parse_bio(bio, finetuned_bio_parser_model))\n",
        "    update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes)  \n",
        "\n",
        "  average_precision = 0\n",
        "  average_recall = 0\n",
        "  total = 0\n",
        "\n",
        "  for attribute in all_attributes:\n",
        "    if all_attributes[attribute] < threshold_count:\n",
        "      continue\n",
        "    print(attribute.upper())\n",
        "    try:\n",
        "      precision = true_positives[attribute] / (true_positives[attribute] + false_positives[attribute])\n",
        "    except: \n",
        "      precision = 0.0\n",
        "    try:\n",
        "      recall = true_positives[attribute] / (true_positives[attribute] + false_negatives[attribute])\n",
        "    except: \n",
        "      recall = 0.0\n",
        "    print(\"precision:\", precision)\n",
        "    print(\"recall:\", recall)\n",
        "    print(\"f-score:\", (precision+recall)/2)\n",
        "    print('---')\n",
        "    average_precision += precision\n",
        "    average_recall += recall\n",
        "    total += 1\n",
        "\n",
        "  print(\"AVERAGE\")\n",
        "  average_precision = average_precision/total\n",
        "  average_recall = average_recall/total\n",
        "  print(\"precision:\", average_precision)\n",
        "  print(\"recall:\", average_recall)\n",
        "  print(\"f-score:\", (average_precision+average_recall)/2)\n",
        "  print('---')\n"
      ],
      "metadata": {
        "id": "M5UXVTgaGHBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to evaluate on the full test set, there are 237 test items.  You can set `max_test_items=237`.  Doing so will call your `parse_bio` function about 237 times, so be sure that you've got it properly debugged and working before running this code. "
      ],
      "metadata": {
        "id": "fBCI95rQI8gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_bio_parser_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "csHnUBTpL1-Q",
        "outputId": "c22890d5-028e-42c5-f9ae-ac70036b3106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'curie:ft-personal-2022-12-07-07-16-32'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset_filename='SynthBio_test.json'\n",
        "max_test_items=10\n",
        "wiki_bio_test = load_wiki_bio_test_set(testset_filename, max_test_items)\n",
        "evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgQ3wjXscxNw",
        "outputId": "2083dfd0-5576-43b4-f494-14cf41d6d760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME\n",
            "precision: 0.4666666666666667\n",
            "recall: 0.7\n",
            "f-score: 0.5833333333333333\n",
            "---\n",
            "GENDER\n",
            "precision: 0.47368421052631576\n",
            "recall: 0.9\n",
            "f-score: 0.6868421052631579\n",
            "---\n",
            "NATIONALITY\n",
            "precision: 0.47058823529411764\n",
            "recall: 0.8\n",
            "f-score: 0.6352941176470588\n",
            "---\n",
            "BIRTH_DATE\n",
            "precision: 0.5\n",
            "recall: 0.6\n",
            "f-score: 0.55\n",
            "---\n",
            "BIRTH_PLACE\n",
            "precision: 0.5833333333333334\n",
            "recall: 0.5384615384615384\n",
            "f-score: 0.5608974358974359\n",
            "---\n",
            "KNOWN_FOR\n",
            "precision: 0.0\n",
            "recall: 0.0\n",
            "f-score: 0.0\n",
            "---\n",
            "ALMA_MATER\n",
            "precision: 0.3\n",
            "recall: 0.375\n",
            "f-score: 0.3375\n",
            "---\n",
            "AWARDS\n",
            "precision: 0.5\n",
            "recall: 0.5555555555555556\n",
            "f-score: 0.5277777777777778\n",
            "---\n",
            "MOTHER\n",
            "precision: 0.4375\n",
            "recall: 0.7\n",
            "f-score: 0.56875\n",
            "---\n",
            "FATHER\n",
            "precision: 0.4\n",
            "recall: 0.6666666666666666\n",
            "f-score: 0.5333333333333333\n",
            "---\n",
            "PARTNER\n",
            "precision: 0.42857142857142855\n",
            "recall: 0.75\n",
            "f-score: 0.5892857142857143\n",
            "---\n",
            "CHILDREN\n",
            "precision: 0.5263157894736842\n",
            "recall: 0.5882352941176471\n",
            "f-score: 0.5572755417956656\n",
            "---\n",
            "NOTABLE_TYPE\n",
            "precision: 0.0\n",
            "recall: 0.0\n",
            "f-score: 0.0\n",
            "---\n",
            "CITIZENSHIP\n",
            "precision: 0.0\n",
            "recall: 0.0\n",
            "f-score: 0.0\n",
            "---\n",
            "OCCUPATION\n",
            "precision: 0.16666666666666666\n",
            "recall: 0.2857142857142857\n",
            "f-score: 0.22619047619047616\n",
            "---\n",
            "DEATH_DATE\n",
            "precision: 0.42857142857142855\n",
            "recall: 0.5\n",
            "f-score: 0.4642857142857143\n",
            "---\n",
            "DEATH_PLACE\n",
            "precision: 0.6666666666666666\n",
            "recall: 0.75\n",
            "f-score: 0.7083333333333333\n",
            "---\n",
            "DEATH_CAUSE\n",
            "precision: 0.25\n",
            "recall: 0.2\n",
            "f-score: 0.225\n",
            "---\n",
            "RESTING_PLACE\n",
            "precision: 0.42857142857142855\n",
            "recall: 0.6\n",
            "f-score: 0.5142857142857142\n",
            "---\n",
            "HOMETOWN\n",
            "precision: 0.0\n",
            "recall: 0.0\n",
            "f-score: 0.0\n",
            "---\n",
            "AVERAGE\n",
            "precision: 0.3513567927170869\n",
            "recall: 0.47548166702578465\n",
            "f-score: 0.41341922987143576\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How well did your model perform?"
      ],
      "metadata": {
        "id": "rCMqucECM9EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - fill in these values\n",
        "average_precision = 0.3513567927170869\n",
        "average_recall = 0.47548166702578465\n",
        "average_fscore = 0.41341922987143576\n",
        "\n",
        "# What attributes had the highest F-scorre\n",
        "best_attributes = {\n",
        "    \"DEATH_PLACE\" : 0.7083333333333333,\n",
        "    \"CHILDREN\" : 0.5572755417956656,\n",
        "}\n",
        "\n",
        "# What attributes had the lowest F-scorre\n",
        "worst_attributes = {\n",
        "    \"HOMETOWN\" : 0.0,\n",
        "    \"DEATH_CAUSE\" : 0.225,\n",
        "}\n",
        "\n",
        "# What could you do the perform the model's performance?\n",
        "potential_improvements = \"\"\"\n",
        "Improve the dataset by increasing the size of what it sees for context.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O5frXHaeNFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback questions"
      ],
      "metadata": {
        "id": "5NFilM6oNv-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many hours did you spend on this assignment? Just an approximation is fine.\n",
        "num_hours_spent = 10\n",
        "\n",
        "# What did you think?  This was the first time we tried this assignment \n",
        "# so you're feedback is valable.\n",
        "feedback = \"\"\"\n",
        "GPT-3 is slow to run, and the notebook I think was a little difficult to follow, but otherwise this was such a cool experience (and a little distopian too). \n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "yPS7_smBN-2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}